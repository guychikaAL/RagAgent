# RagAgentv2 - Complete Project Overview

## ğŸ“‹ **Table of Contents**

1. [What is This Project?](#what-is-this-project)
2. [Detailed Version](#detailed-version)
   - [Flow 1: Build Production Index](#flow-1-build-production-index-build-time)
   - [Flow 2: Query Time](#flow-2-query-time-answering-questions)
   - [Evaluation Systems](#evaluation-systems)
3. [Short Version](#short-version)
4. [Overall Project Summary](#overall-project-summary)

---

## ğŸ¯ **What is This Project?**

**RagAgentv2** is a **production-ready Retrieval-Augmented Generation (RAG) system** specifically designed for **insurance claim processing**. It allows users to ask natural language questions about insurance claim documents and receive accurate, grounded answers.

**Core Capabilities:**
- Process multi-claim PDF documents (20+ claims per file)
- Answer atomic questions ("What is Jon Mor's phone?")
- Answer complex questions ("Summarize the claim timeline")
- Perform date calculations using MCP tools ("How many days from accident to repair?")
- Prevent hallucination through strict grounding
- Evaluate system performance with dual evaluation frameworks

---

# ğŸ”„ **FLOWS SECTION**

## **Overview: Two Critical Flows**

The RagAgentv2 system operates through **two distinct flows** that work together to enable fast, accurate question answering:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  FLOW 1: BUILD PRODUCTION INDEX (One-Time)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  Raw PDF â†’ Searchable Vector Database                   â”‚
â”‚  Duration: 5-10 minutes                                 â”‚
â”‚  Frequency: Once (or when data changes)                 â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  FLOW 2: QUERY TIME (Every Question)                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  User Question â†’ Intelligent Answer                     â”‚
â”‚  Duration: 2-5 seconds                                  â”‚
â”‚  Frequency: Every user query                            â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **FLOW 1: Build Production Index (Build Time)**

### **Complete Flow Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FLOW 1: BUILD PRODUCTION INDEX                         â”‚
â”‚                      (Run Once - ~5-10 minutes)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  ğŸ“„ INPUT: auto_claim_20_forms_FINAL.pdf                                â”‚
â”‚            (45 pages, 20 insurance claims)                               â”‚
â”‚                                                                          â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 1.1: PDF INGESTION                                       â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: RAG/PDF_Ingestion/pdf_ingestion.py                       â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Validate PDF file (exists, readable, not encrypted)         â”‚    â”‚
â”‚  â”‚  2. Extract text from all 45 pages                              â”‚    â”‚
â”‚  â”‚  3. Remove page numbers and artifacts                           â”‚    â”‚
â”‚  â”‚  4. Fix broken line breaks (automo-\nbile â†’ automobile)         â”‚    â”‚
â”‚  â”‚  5. Normalize whitespace and reconstruct paragraphs             â”‚    â”‚
â”‚  â”‚  6. Extract metadata (pages, words, dates, etc.)                â”‚    â”‚
â”‚  â”‚  7. Create LlamaIndex Document object                           â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: 1 Document (clean text + metadata)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 1.2: CLAIM SEGMENTATION                                  â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: RAG/Claim_Segmentation/claim_segmentation.py             â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Detect claim boundaries ("AUTO CLAIM FORM #N")              â”‚    â”‚
â”‚  â”‚  2. Extract text slice for each claim                           â”‚    â”‚
â”‚  â”‚  3. Extract claimant name dynamically (e.g., "Jon Mor")         â”‚    â”‚
â”‚  â”‚  4. Generate unique claim_id for each claim                     â”‚    â”‚
â”‚  â”‚  5. Add claim-specific metadata                                 â”‚    â”‚
â”‚  â”‚  6. Create separate Document for each claim                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: 20 Documents (one per claim)                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 1.3: CHUNKING LAYER                                      â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: RAG/Chunking_Layer/chunking_layer.py                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Detect sections (SECTION N â€“ TITLE patterns)                â”‚    â”‚
â”‚  â”‚     â””â”€ Create IndexNode for each section                        â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Create parent chunks (~800 characters each)                 â”‚    â”‚
â”‚  â”‚     â””â”€ Prepend claim context to each parent                     â”‚    â”‚
â”‚  â”‚     â””â”€ Create TextNode for each parent                          â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Create child chunks (~200 characters each)                  â”‚    â”‚
â”‚  â”‚     â””â”€ Split parents into smaller chunks                        â”‚    â”‚
â”‚  â”‚     â””â”€ Create TextNode for each child                           â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. Link relationships (section â†’ parent â†’ child)               â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  5. Enrich metadata (chunk_id, claim_id, position, etc.)        â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Hierarchy: 3 Levels                                            â”‚    â”‚
â”‚  â”‚  â€¢ Sections: Navigational structure                             â”‚    â”‚
â”‚  â”‚  â€¢ Parents: Broad context (~800 chars)                          â”‚    â”‚
â”‚  â”‚  â€¢ Children: Precise facts (~200 chars)                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: ~550 Hierarchical Nodes                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 1.4: INDEX LAYER                                         â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: RAG/Index_Layer/index_layer.py                           â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Initialize embedding model                                  â”‚    â”‚
â”‚  â”‚     â€¢ Model: text-embedding-3-small                             â”‚    â”‚
â”‚  â”‚     â€¢ Dimension: 1536                                           â”‚    â”‚
â”‚  â”‚     â€¢ CRITICAL: Same model for build AND query                  â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Embed all 550 nodes                                         â”‚    â”‚
â”‚  â”‚     â€¢ API call for each node â†’ 1536-dim vector                  â”‚    â”‚
â”‚  â”‚     â€¢ Vectors capture semantic meaning                          â”‚    â”‚
â”‚  â”‚     â€¢ Main time cost: ~5 minutes (API calls)                    â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Build FAISS vector store                                    â”‚    â”‚
â”‚  â”‚     â€¢ Fast similarity search index                              â”‚    â”‚
â”‚  â”‚     â€¢ Stores embeddings + metadata                              â”‚    â”‚
â”‚  â”‚     â€¢ Enables sub-second retrieval                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. Create storage context                                      â”‚    â”‚
â”‚  â”‚     â€¢ docstore: Original node texts                             â”‚    â”‚
â”‚  â”‚     â€¢ vector_store: Embeddings                                  â”‚    â”‚
â”‚  â”‚     â€¢ index_store: Relationships                                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  5. Build VectorStoreIndex                                      â”‚    â”‚
â”‚  â”‚     â€¢ Combines vector store + storage                           â”‚    â”‚
â”‚  â”‚     â€¢ Handles query-time search                                 â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  6. Build SummaryIndex                                          â”‚    â”‚
â”‚  â”‚     â€¢ For comprehensive retrieval (no filtering)                â”‚    â”‚
â”‚  â”‚     â€¢ Used by MapReduce                                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  7. Create retrievers                                           â”‚    â”‚
â”‚  â”‚     â€¢ Needle Retriever: top_k=3, threshold=0.75                 â”‚    â”‚
â”‚  â”‚       â†’ For atomic questions (high precision)                   â”‚    â”‚
â”‚  â”‚     â€¢ MapReduce Engine: top_k=15                                â”‚    â”‚
â”‚  â”‚       â†’ For complex questions (high recall)                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  8. Save to disk                                                â”‚    â”‚
â”‚  â”‚     â€¢ production_index/docstore.json                            â”‚    â”‚
â”‚  â”‚     â€¢ production_index/vector_store.json                        â”‚    â”‚
â”‚  â”‚     â€¢ production_index/index_store.json                         â”‚    â”‚
â”‚  â”‚     â€¢ production_index/default__vector_store.json               â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: production_index/ folder                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  ğŸ’¾ OUTPUT: production_index/ (ready for query time!)                   â”‚
â”‚                                                                          â”‚
â”‚  âœ… RESULT:                                                              â”‚
â”‚     â€¢ 550 embedded nodes stored in FAISS                                â”‚
â”‚     â€¢ Fast similarity search enabled                                    â”‚
â”‚     â€¢ Ready to answer questions in seconds                              â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Flow 1 Stage Summary:**

| Stage | File | Input | Output | Duration |
|-------|------|-------|--------|----------|
| 1.1 PDF Ingestion | `pdf_ingestion.py` | PDF file | 1 Document | ~30 sec |
| 1.2 Claim Segmentation | `claim_segmentation.py` | 1 Document | 20 Documents | ~5 sec |
| 1.3 Chunking | `chunking_layer.py` | 20 Documents | 550 Nodes | ~10 sec |
| 1.4 Indexing | `index_layer.py` | 550 Nodes | production_index/ | ~5-10 min |
| **Total** | | **PDF** | **Searchable Index** | **~5-10 min** |

---

## **FLOW 2: Query Time (Answering Questions)**

### **Complete Flow Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FLOW 2: QUERY TIME                                   â”‚
â”‚                 (Every Question - ~2-5 seconds)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  ğŸ’¬ INPUT: User Question                                                â”‚
â”‚            "What is Jon Mor's phone number?"                             â”‚
â”‚                                                                          â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.1: LOAD PRODUCTION INDEX                               â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: main.py â†’ index_layer.py                                 â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Read production_index/ folder from disk                     â”‚    â”‚
â”‚  â”‚  2. Load docstore.json (node texts)                             â”‚    â”‚
â”‚  â”‚  3. Load vector_store.json (embeddings)                         â”‚    â”‚
â”‚  â”‚  4. Load index_store.json (relationships)                       â”‚    â”‚
â”‚  â”‚  5. Reconstruct FAISS index in memory                           â”‚    â”‚
â”‚  â”‚  6. Recreate VectorStoreIndex                                   â”‚    â”‚
â”‚  â”‚  7. Recreate SummaryIndex                                       â”‚    â”‚
â”‚  â”‚  8. Initialize embedding model (SAME as build time)             â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: ~2 seconds (no API calls, just file loading)         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Index loaded in memory, ready for queries              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.2: INITIALIZE AGENTS                                   â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: main.py                                                  â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Create Router Agent                                         â”‚    â”‚
â”‚  â”‚     â€¢ LLM: gpt-4o-mini, temp=0.0                                â”‚    â”‚
â”‚  â”‚     â€¢ Purpose: Classify question type                           â”‚    â”‚
â”‚  â”‚     â€¢ Output: "needle" or "summary" route                       â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Create Needle Agent                                         â”‚    â”‚
â”‚  â”‚     â€¢ LLM: gpt-4o-mini, temp=0.0                                â”‚    â”‚
â”‚  â”‚     â€¢ Purpose: Extract atomic facts                             â”‚    â”‚
â”‚  â”‚     â€¢ Features: MCP tools enabled, null-safe                    â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Create Summary Agent                                        â”‚    â”‚
â”‚  â”‚     â€¢ LLM: gpt-4o-mini, temp=0.2                                â”‚    â”‚
â”‚  â”‚     â€¢ Purpose: Synthesize comprehensive answers                 â”‚    â”‚
â”‚  â”‚     â€¢ Features: MCP tools enabled, MapReduce support            â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. Get retrievers from Index Layer                             â”‚    â”‚
â”‚  â”‚     â€¢ Needle Retriever (top_k=3, threshold=0.75)                â”‚    â”‚
â”‚  â”‚     â€¢ MapReduce Query Engine (top_k=15)                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: <1 second (LLM initialization)                       â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Three agents ready, retrievers configured              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.3: ORCHESTRATOR INITIALIZATION                         â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: main.py â†’ orchestrator.py                                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Inject all dependencies                                     â”‚    â”‚
â”‚  â”‚     â€¢ Router Agent                                              â”‚    â”‚
â”‚  â”‚     â€¢ Needle Agent                                              â”‚    â”‚
â”‚  â”‚     â€¢ Summary Agent                                             â”‚    â”‚
â”‚  â”‚     â€¢ Needle Retriever                                          â”‚    â”‚
â”‚  â”‚     â€¢ MapReduce Engine                                          â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Validate all components present                             â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Create orchestrator instance                                â”‚    â”‚
â”‚  â”‚     â€¢ Pure coordinator (no business logic)                      â”‚    â”‚
â”‚  â”‚     â€¢ Stateless (no memory between queries)                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: <1 second                                            â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Orchestrator ready to handle queries                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.4: QUERY PREPROCESSING                                 â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: orchestrator.py (run method)                             â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Analyze user question for claim identifiers                 â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Extract claim number (if present)                           â”‚    â”‚
â”‚  â”‚     â€¢ Patterns: "claim #5", "form number 5"                     â”‚    â”‚
â”‚  â”‚     â€¢ Result: claim_number = "5"                                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Extract claimant name (if present)                          â”‚    â”‚
â”‚  â”‚     â€¢ Pattern: Capitalized first + last name                    â”‚    â”‚
â”‚  â”‚     â€¢ Example: "Jon Mor's phone" â†’ "Jon Mor"                    â”‚    â”‚
â”‚  â”‚     â€¢ Result: claimant_name = "Jon Mor"                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. Create PostFilterRetriever (if needed)                      â”‚    â”‚
â”‚  â”‚     â€¢ If claim identifier detected:                             â”‚    â”‚
â”‚  â”‚       â†’ Wrap base retriever                                     â”‚    â”‚
â”‚  â”‚       â†’ Retrieve 3x more results                                â”‚    â”‚
â”‚  â”‚       â†’ Filter by metadata (claim_number OR claimant_name)      â”‚    â”‚
â”‚  â”‚       â†’ Return top_k after filtering                            â”‚    â”‚
â”‚  â”‚     â€¢ Why: FAISS doesn't support native metadata filtering      â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: <100ms                                               â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Filtered or default retriever ready                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.5: ROUTING DECISION                                    â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: orchestrator.py â†’ router_agent.py                        â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Send question to Router Agent                               â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Router LLM analyzes question intent                         â”‚    â”‚
â”‚  â”‚     â€¢ No retrieval at this stage (pure classification)          â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Classification logic:                                       â”‚    â”‚
â”‚  â”‚     â€¢ NEEDLE: Single specific fact needed                       â”‚    â”‚
â”‚  â”‚       Examples: "What's the phone?", "When accident?"           â”‚    â”‚
â”‚  â”‚       Also: Date calculations ("How many days?")                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚     â€¢ SUMMARY: Multiple facts or explanation                    â”‚    â”‚
â”‚  â”‚       Examples: "Summarize claim", "What happened?"             â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. Return route decision                                       â”‚    â”‚
â”‚  â”‚     â€¢ route: "needle" or "summary"                              â”‚    â”‚
â”‚  â”‚     â€¢ confidence: 0.0 to 1.0                                    â”‚    â”‚
â”‚  â”‚     â€¢ reason: Explanation                                       â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Example for "What is Jon Mor's phone?":                        â”‚    â”‚
â”‚  â”‚     route = "needle"                                            â”‚    â”‚
â”‚  â”‚     confidence = 0.95                                           â”‚    â”‚
â”‚  â”‚     reason = "Asks for single specific fact"                    â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: ~500ms (LLM API call)                                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Routing decision (which agent to use)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.6a: NEEDLE AGENT EXECUTION (if route = "needle")       â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: orchestrator.py â†’ needle_agent.py                        â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. RETRIEVAL                                                   â”‚    â”‚
â”‚  â”‚     â€¢ Use Needle Retriever (top_k=3, threshold=0.75)            â”‚    â”‚
â”‚  â”‚     â€¢ Embed user question                                       â”‚    â”‚
â”‚  â”‚     â€¢ Cosine similarity search in FAISS                         â”‚    â”‚
â”‚  â”‚     â€¢ Return 3 most similar chunks (if above threshold)         â”‚    â”‚
â”‚  â”‚     â€¢ Chunks are ~200 chars (child chunks)                      â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. CONTEXT PREPARATION                                         â”‚    â”‚
â”‚  â”‚     â€¢ Format chunks for LLM                                     â”‚    â”‚
â”‚  â”‚     â€¢ Include metadata (claim_id, claimant_name)                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3a. STANDARD PATH (No date calculation)                        â”‚    â”‚
â”‚  â”‚      â€¢ Send question + chunks to Needle LLM                     â”‚    â”‚
â”‚  â”‚      â€¢ LLM extracts exact answer                                â”‚    â”‚
â”‚  â”‚      â€¢ Returns structured response                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3b. MCP TOOL PATH (Date calculation needed)                    â”‚    â”‚
â”‚  â”‚      â€¢ LLM recognizes date calculation in question              â”‚    â”‚
â”‚  â”‚      â€¢ Extracts dates from chunks                               â”‚    â”‚
â”‚  â”‚      â€¢ Tool Call Decision (tool_choice="auto"):                 â”‚    â”‚
â”‚  â”‚        â†’ LLM: "I see two dates, call calculate_days_between"    â”‚    â”‚
â”‚  â”‚      â€¢ Tool Execution:                                          â”‚    â”‚
â”‚  â”‚        â†’ calculate_days_between("2024-01-24", "2024-02-18")     â”‚    â”‚
â”‚  â”‚        â†’ Python datetime performs exact calculation             â”‚    â”‚
â”‚  â”‚        â†’ Returns: {"success": True, "number_of_days": 25}       â”‚    â”‚
â”‚  â”‚      â€¢ Final Answer Formation:                                  â”‚    â”‚
â”‚  â”‚        â†’ LLM receives tool result                               â”‚    â”‚
â”‚  â”‚        â†’ Formats: "25 days passed between..."                   â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. RESPONSE FORMATION                                          â”‚    â”‚
â”‚  â”‚     â€¢ answer: Extracted fact or "null"                          â”‚    â”‚
â”‚  â”‚     â€¢ confidence: 1.0 if found, 0.0 if not                      â”‚    â”‚
â”‚  â”‚     â€¢ sources: List of chunk IDs                                â”‚    â”‚
â”‚  â”‚     â€¢ reason: Explanation (mentions MCP if used)                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Example Response:                                              â”‚    â”‚
â”‚  â”‚     answer = "555-1234"                                         â”‚    â”‚
â”‚  â”‚     confidence = 1.0                                            â”‚    â”‚
â”‚  â”‚     sources = ["chunk_abc123"]                                  â”‚    â”‚
â”‚  â”‚     reason = "Found in contact section"                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: ~1-2 seconds (retrieval + LLM)                       â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Precise answer with high confidence                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â”‚                                                                    â”‚
â”‚     OR                                                                   â”‚
â”‚     â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.6b: SUMMARY AGENT EXECUTION (if route = "summary")     â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: orchestrator.py â†’ summary_agent.py                       â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process (MapReduce Approach):                                  â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  1. RETRIEVAL PHASE                                             â”‚    â”‚
â”‚  â”‚     â€¢ Use MapReduce Query Engine                                â”‚    â”‚
â”‚  â”‚     â€¢ Retrieve top_k=15 chunks (comprehensive)                  â”‚    â”‚
â”‚  â”‚     â€¢ Uses both parent and child chunks                         â”‚    â”‚
â”‚  â”‚     â€¢ No similarity threshold (high recall)                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. MAP PHASE                                                   â”‚    â”‚
â”‚  â”‚     â€¢ For each of 15 retrieved chunks:                          â”‚    â”‚
â”‚  â”‚       â†’ LLM generates summary of that chunk                     â”‚    â”‚
â”‚  â”‚       â†’ Focuses on question-relevant information                â”‚    â”‚
â”‚  â”‚     â€¢ Results in 15 mini-summaries                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. REDUCE PHASE                                                â”‚    â”‚
â”‚  â”‚     â€¢ LLM combines all mini-summaries                           â”‚    â”‚
â”‚  â”‚     â€¢ Synthesizes coherent final answer                         â”‚    â”‚
â”‚  â”‚     â€¢ Resolves contradictions                                   â”‚    â”‚
â”‚  â”‚     â€¢ Organizes information logically                           â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. MCP TOOL INTEGRATION (if needed)                            â”‚    â”‚
â”‚  â”‚     â€¢ During synthesis, LLM may recognize date calculation      â”‚    â”‚
â”‚  â”‚     â€¢ Calls calculate_days_between with dates from context      â”‚    â”‚
â”‚  â”‚     â€¢ Incorporates exact calculation in final answer            â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  5. RESPONSE FORMATION                                          â”‚    â”‚
â”‚  â”‚     â€¢ answer: Comprehensive synthesized response                â”‚    â”‚
â”‚  â”‚     â€¢ confidence: 0.8-0.9 (synthesis less certain)              â”‚    â”‚
â”‚  â”‚     â€¢ sources: All chunk IDs used (15+)                         â”‚    â”‚
â”‚  â”‚     â€¢ reason: Explanation of synthesis                          â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Example Response:                                              â”‚    â”‚
â”‚  â”‚     answer = "Jon Mor filed a claim on Jan 26, 2024,            â”‚    â”‚
â”‚  â”‚               following an accident on Jan 24, 2024..."         â”‚    â”‚
â”‚  â”‚     confidence = 0.9                                            â”‚    â”‚
â”‚  â”‚     sources = [15+ chunk IDs]                                   â”‚    â”‚
â”‚  â”‚     reason = "Synthesized from incident and payment sections"   â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: ~2-4 seconds (retrieval + multiple LLM calls)        â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Comprehensive answer with broad context                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  STAGE 2.7: RESPONSE NORMALIZATION                              â”‚    â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚    â”‚
â”‚  â”‚  File: orchestrator.py (run method)                             â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Process:                                                        â”‚    â”‚
â”‚  â”‚  1. Combine routing metadata + agent result                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. Create unified response structure:                          â”‚    â”‚
â”‚  â”‚     {                                                            â”‚    â”‚
â”‚  â”‚       "route": "needle" or "summary",                           â”‚    â”‚
â”‚  â”‚       "answer": "555-1234",                                     â”‚    â”‚
â”‚  â”‚       "confidence": 1.0,                                        â”‚    â”‚
â”‚  â”‚       "sources": ["chunk_abc123", ...],                         â”‚    â”‚
â”‚  â”‚       "retrieved_chunks_content": ["Phone: 555-1234", ...],     â”‚    â”‚
â”‚  â”‚       "reason": "Found in contact section"                      â”‚    â”‚
â”‚  â”‚     }                                                            â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. Log results:                                                â”‚    â”‚
â”‚  â”‚     â€¢ Print route decision                                      â”‚    â”‚
â”‚  â”‚     â€¢ Print final answer                                        â”‚    â”‚
â”‚  â”‚     â€¢ Print confidence score                                    â”‚    â”‚
â”‚  â”‚     â€¢ Print number of sources                                   â”‚    â”‚
â”‚  â”‚     â€¢ Print reasoning                                           â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Duration: <100ms                                               â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Output: Standardized response for external consumers           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â†“                                                                     â”‚
â”‚  âœ… OUTPUT: Final Answer                                                â”‚
â”‚            "555-1234"                                                    â”‚
â”‚                                                                          â”‚
â”‚  ğŸ“Š TOTAL TIME: ~2-5 seconds (fast!)                                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Flow 2 Stage Summary:**

| Stage | File | Purpose | Duration |
|-------|------|---------|----------|
| 2.1 Load Index | `index_layer.py` | Load pre-built index from disk | ~2 sec |
| 2.2 Initialize Agents | `main.py` | Create Router, Needle, Summary agents | <1 sec |
| 2.3 Orchestrator Init | `orchestrator.py` | Create central coordinator | <1 sec |
| 2.4 Query Preprocessing | `orchestrator.py` | Extract claim identifiers, create filters | <100ms |
| 2.5 Routing | `router_agent.py` | Classify question (needle/summary) | ~500ms |
| 2.6a Needle Execution | `needle_agent.py` | Extract atomic fact (+ MCP if needed) | ~1-2 sec |
| 2.6b Summary Execution | `summary_agent.py` | Synthesize comprehensive answer (MapReduce) | ~2-4 sec |
| 2.7 Response Normalization | `orchestrator.py` | Format unified response | <100ms |
| **Total** | | **Question â†’ Answer** | **~2-5 sec** |

---

### **Flow 2 Decision Tree:**

```
User Question
    â†“
Load Index â†’ Initialize Agents â†’ Orchestrator
    â†“
Query Preprocessing
    â”œâ”€ Claim Number? â†’ Filter by claim_number
    â”œâ”€ Claimant Name? â†’ Filter by claimant_name
    â””â”€ No identifier â†’ Use default retriever
    â†“
Routing Decision
    â”œâ”€ Route = NEEDLE
    â”‚   â†“
    â”‚   Needle Agent
    â”‚   â”œâ”€ Retrieve 3 chunks (threshold=0.75)
    â”‚   â”œâ”€ Date calculation needed?
    â”‚   â”‚   â”œâ”€ YES â†’ Call MCP tool â†’ Format answer
    â”‚   â”‚   â””â”€ NO â†’ Extract fact â†’ Return answer
    â”‚   â””â”€ Return precise answer
    â”‚
    â””â”€ Route = SUMMARY
        â†“
        Summary Agent
        â”œâ”€ Retrieve 15 chunks (no threshold)
        â”œâ”€ Map Phase: Summarize each chunk
        â”œâ”€ Reduce Phase: Combine summaries
        â”œâ”€ Date calculation needed?
        â”‚   â”œâ”€ YES â†’ Call MCP tool â†’ Incorporate in answer
        â”‚   â””â”€ NO â†’ Return synthesized answer
        â””â”€ Return comprehensive answer
    â†“
Response Normalization
    â†“
Final Answer to User
```

---

### **Key Differences Between Flows:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FLOW 1 vs. FLOW 2 COMPARISON                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  FLOW 1 (Build Index):                                  â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ When: Once (or when data changes)                    â”‚
â”‚  â€¢ Duration: 5-10 minutes                               â”‚
â”‚  â€¢ Main Cost: Embedding API calls (550 chunks)          â”‚
â”‚  â€¢ Output: production_index/ folder on disk             â”‚
â”‚  â€¢ Purpose: Prepare data for fast retrieval             â”‚
â”‚  â€¢ Stages: 4 (Ingest â†’ Segment â†’ Chunk â†’ Index)        â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  FLOW 2 (Query Time):                                   â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ When: Every user question                            â”‚
â”‚  â€¢ Duration: 2-5 seconds                                â”‚
â”‚  â€¢ Main Cost: LLM API calls (1-3 per query)             â”‚
â”‚  â€¢ Output: Natural language answer                      â”‚
â”‚  â€¢ Purpose: Answer questions fast                       â”‚
â”‚  â€¢ Stages: 7 (Load â†’ Init â†’ Route â†’ Execute â†’ Answer)  â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  WHY THIS DESIGN:                                       â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  âœ… Efficiency: Build once, query many times            â”‚
â”‚  âœ… Speed: Query time is fast (no re-embedding)         â”‚
â”‚  âœ… Cost: Embedding cost paid once, not per query       â”‚
â”‚  âœ… Scalability: Supports concurrent users              â”‚
â”‚  âœ… Production-Ready: Proven architecture pattern       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“– **Detailed Version**

## **System Architecture: Two Flows**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TWO DISTINCT FLOWS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  FLOW 1: BUILD PRODUCTION INDEX                         â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  When: Once (or when data changes)                      â”‚
â”‚  File: build_production_index.py                        â”‚
â”‚  Purpose: Transform PDF â†’ Searchable Index              â”‚
â”‚  Duration: ~5-10 minutes                                â”‚
â”‚  Output: production_index/ folder                       â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  FLOW 2: QUERY TIME                                     â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  When: Every user question                              â”‚
â”‚  File: main.py                                          â”‚
â”‚  Purpose: Answer questions using pre-built index        â”‚
â”‚  Duration: 2-5 seconds per question                     â”‚
â”‚  Output: Natural language answer                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **Flow 1: Build Production Index (Build Time)**

**Purpose:** Transform a raw PDF document into a searchable, embedded vector database that enables fast, accurate retrieval.

**File:** `build_production_index.py`

**When to Run:** Once initially, then again only when:
- New PDF data is added
- Existing data changes
- Index configuration needs updating

---

### **Stage 1.1: PDF Ingestion**

**Location:** `RAG/PDF_Ingestion/pdf_ingestion.py`

**What it does:**
Converts raw PDF files into clean, normalized text documents ready for processing.

**Detailed Process:**

1. **File Validation:**
   - Checks PDF exists and is readable
   - Verifies file size (< 100MB)
   - Ensures not encrypted
   - Validates .pdf extension

2. **Text Extraction:**
   - Opens PDF with pypdf library
   - Extracts text from all pages (45 pages in our case)
   - Removes page numbers and artifacts
   - Fixes broken line breaks (e.g., "automo-\nbile" â†’ "automobile")

3. **Text Normalization:**
   - Removes control characters (form feeds, carriage returns)
   - Normalizes whitespace (multiple spaces â†’ single space)
   - Reconstructs paragraphs (joins lines within paragraphs)
   - Collapses multiple newlines

4. **Metadata Extraction:**
   - Generates deterministic document_id (hash-based)
   - Extracts title (from first line or filename)
   - Counts pages, words, paragraphs
   - Detects dates and times in document
   - Calculates numeric density (helps identify tables vs. prose)
   - Detects heading structure

5. **Document Creation:**
   - Creates LlamaIndex Document object
   - Attaches clean text
   - Attaches metadata
   - Sets deterministic ID

**Output:** Single `Document` object with clean text and metadata

**Why Important:** Clean, normalized text is critical for accurate chunking and retrieval. Bad text extraction = bad retrieval.

---

### **Stage 1.2: Claim Segmentation**

**Location:** `RAG/Claim_Segmentation/claim_segmentation.py`

**What it does:**
Splits one PDF containing multiple insurance claims into separate documents, one per claim.

**Detailed Process:**

1. **Boundary Detection:**
   - Scans document for claim markers
   - Primary pattern: "AUTO CLAIM FORM #N"
   - Fallback patterns: "Claim Number:", section headers
   - Records position of each boundary in the text

2. **Text Slicing:**
   - For each detected boundary:
     - Start position = boundary location
     - End position = next boundary (or end of document)
   - Extracts text slice for each claim

3. **Claimant Name Extraction (Dynamic):**
   - Looks for "Name: FirstName LastName" pattern
   - Extracts from first 500 characters of claim
   - Handles various formats (with/without newlines)
   - NO hardcoding - extracted dynamically from document

4. **Metadata Enrichment:**
   - Generates unique claim_id (hash-based)
   - Adds claim_number (from form)
   - Adds claimant_name (extracted)
   - Inherits parent document metadata
   - Adds claim-specific statistics

5. **Document Creation:**
   - Creates separate Document for each claim
   - Each claim becomes independent processing unit

**Input:** 1 Document (entire PDF)
**Output:** 20 Documents (one per claim)

**Why Important:** Insurance claims are independent business entities. Mixing claims during retrieval causes hallucination ("Jon Mor's phone" should never retrieve Jane Smith's phone). Segmentation ensures claim-level isolation.

---

### **Stage 1.3: Chunking Layer**

**Location:** `RAG/Chunking_Layer/chunking_layer.py`

**What it does:**
Transforms each claim document into a **3-level hierarchical structure** optimized for different retrieval strategies.

**Detailed Process:**

1. **Section Detection:**
   - Detects major sections using heuristic patterns
   - Looks for: "SECTION N â€“ TITLE" patterns
   - Common sections: Claimant Info, Incident Details, Vehicle Info, etc.
   - Creates IndexNode for each section (navigational structure)

2. **Parent Chunking:**
   - Splits each section into ~800-character parent chunks
   - Maintains semantic coherence (doesn't break mid-sentence)
   - Prepends claim context to each parent chunk
   - Example context: "Claim #1 (Jon Mor) - Claimant Information:"
   - Creates TextNode for each parent chunk

3. **Child Chunking:**
   - Further splits each parent into ~200-character child chunks
   - More granular for precise retrieval
   - Inherits parent context
   - Creates TextNode for each child chunk

4. **Relationship Linking:**
   - Links sections to their parent chunks
   - Links parent chunks to their child chunks
   - Maintains hierarchical references in metadata

5. **Metadata Enrichment:**
   - Each node gets:
     - chunk_id (unique identifier)
     - claim_id (which claim it belongs to)
     - claimant_name (for filtering)
     - chunk_type (section/parent/child)
     - position (order in document)
     - parent_id, section_id (hierarchy)
     - semantic_features (section name, etc.)

**Hierarchy Example:**
```
Claim #1 (Jon Mor)
  â””â”€ Section: Claimant Information [IndexNode]
      â””â”€ Parent Chunk 1: "Claim #1 (Jon Mor) - Claimant Information: Name: Jon Mor..." [TextNode]
          â”œâ”€ Child Chunk 1.1: "Name: Jon Mor, Phone: 555-1234" [TextNode]
          â””â”€ Child Chunk 1.2: "Address: 123 Main St" [TextNode]
      â””â”€ Parent Chunk 2: "Claim #1 (Jon Mor) - Claimant Information: Account Number..." [TextNode]
          â”œâ”€ Child Chunk 2.1: "Account: 123456" [TextNode]
          â””â”€ Child Chunk 2.2: "Email: jon@example.com" [TextNode]
```

**Input:** 20 Claim Documents
**Output:** ~550 Hierarchical Nodes (sections, parents, children)

**Why Important:** Different questions need different granularity. Atomic questions ("What's the phone?") need small, precise chunks (children). Complex questions ("Summarize the claim") need broader context (parents + children). Hierarchy enables both.

---

### **Stage 1.4: Index Layer**

**Location:** `RAG/Index_Layer/index_layer.py`

**What it does:**
Converts hierarchical nodes into searchable vector embeddings and builds FAISS indexes for fast similarity search.

**Detailed Process:**

1. **Embedding Model Initialization:**
   - Creates single OpenAIEmbedding instance
   - Model: `text-embedding-3-small`
   - Dimension: 1536
   - **Critical Rule:** Same embedding model for build AND query
   - Why: Different models produce incompatible vector spaces

2. **Node Embedding:**
   - For each of 550 nodes:
     - Send node text to OpenAI embedding API
     - Receive 1536-dimensional vector
     - Vector represents semantic meaning of text
   - Batch processing for efficiency
   - API calls are main time cost (~5 minutes)

3. **FAISS Vector Store Creation:**
   - Creates FAISS index (Facebook AI Similarity Search)
   - Index type: Flat (exact search, not approximate)
   - Stores: vector embeddings + metadata
   - In-memory structure, persisted to disk
   - Enables fast cosine similarity search

4. **Storage Context Creation:**
   - Creates docstore (stores original node text)
   - Creates index_store (stores node relationships)
   - Creates vector_store (stores embeddings)
   - All three components work together

5. **VectorStoreIndex Building:**
   - Combines vector store + storage context
   - Creates LlamaIndex VectorStoreIndex
   - Handles query-time embedding + search
   - Returns similar nodes for any query

6. **SummaryIndex Building:**
   - Creates separate index for comprehensive retrieval
   - Uses all nodes (no similarity filtering)
   - Used for MapReduce summarization

7. **Retriever Creation:**
   - **Needle Retriever:**
     - Configuration: top_k=3, similarity_threshold=0.75
     - Retrieves few, highly relevant chunks
     - For atomic questions ("What's the phone?")
     - High precision, low recall
   
   - **Summary Retriever:**
     - Configuration: top_k=8, no threshold
     - Retrieves more chunks for context
     - For complex questions
     - High recall, moderate precision

8. **MapReduce Query Engine Creation:**
   - Uses SummaryIndex
   - Retrieves many chunks (top_k=15)
   - Hierarchical summarization:
     - Map: Summarize each chunk individually
     - Reduce: Combine summaries into final answer
   - For comprehensive questions

9. **Persistence:**
   - Saves to `production_index/` folder:
     - `docstore.json` (node texts)
     - `vector_store.json` (embeddings)
     - `index_store.json` (relationships)
     - `default__vector_store.json` (FAISS index)

**Input:** 550 Hierarchical Nodes
**Output:** 
- `production_index/` folder on disk
- Ready for fast query-time loading

**Why Important:** Embeddings transform text into mathematical vectors that capture semantic similarity. "Phone number" and "contact info" have similar vectors even with different words. FAISS enables sub-second retrieval from 550 chunks.

---

### **Flow 1 Summary:**

```
PDF File (45 pages, 20 claims)
    â†“
[PDF Ingestion]
    â†’ Clean text + metadata
    â†“
[Claim Segmentation]
    â†’ 20 separate claim documents
    â†“
[Chunking Layer]
    â†’ 550 hierarchical nodes (sections, parents, children)
    â†“
[Index Layer]
    â†’ 550 embedded vectors + FAISS index
    â†“
production_index/ folder
    â†’ Ready for query time!
```

**Result:** A pre-built, optimized index that enables fast, accurate retrieval for any user question.

---

## **Flow 2: Query Time (Answering Questions)**

**Purpose:** Use the pre-built index to answer user questions quickly and accurately.

**File:** `main.py`

**When to Run:** Every time a user asks a question (can handle many concurrent users).

---

### **Stage 2.1: Load Production Index**

**Location:** `main.py` â†’ `index_layer.py`

**What it does:**
Loads the pre-built index from disk into memory for fast retrieval.

**Detailed Process:**

1. **Index Loading:**
   - Reads from `production_index/` folder
   - Loads docstore.json (node texts)
   - Loads vector_store.json (embeddings)
   - Loads index_store.json (relationships)
   - Reconstructs FAISS index in memory

2. **Storage Context Recreation:**
   - Recreates docstore
   - Recreates vector_store
   - Recreates index_store
   - Links all components

3. **Index Reconstruction:**
   - Recreates VectorStoreIndex
   - Recreates SummaryIndex
   - Both reference same underlying storage

4. **Embedding Model Initialization:**
   - Creates same OpenAIEmbedding instance as build time
   - **Critical:** Must be SAME model as used during building
   - Model: `text-embedding-3-small`
   - Used to embed user queries

**Duration:** ~2 seconds (much faster than building!)

**Why Fast:** No API calls needed. Just loading files from disk and reconstructing in-memory structures.

---

### **Stage 2.2: Initialize Agents**

**Location:** `main.py`

**What it does:**
Creates the three AI agents that power the RAG system.

**Detailed Process:**

1. **Router Agent Initialization:**
   - LLM: OpenAI gpt-4o-mini
   - Temperature: 0.0 (deterministic)
   - Purpose: Classify question type
   - Output: "needle" or "summary" route
   - System prompt includes classification rules:
     - NEEDLE: Atomic facts, specific data, date calculations
     - SUMMARY: Complex questions, explanations, timelines

2. **Needle Agent Initialization:**
   - LLM: OpenAI gpt-4o-mini
   - Temperature: 0.0 (precise facts)
   - Purpose: Extract atomic facts from context
   - Features:
     - Structured output (Pydantic models)
     - MCP tools enabled (for date calculations)
     - Null-safe (can return null if not found)
   - System prompt: Extract exact answer, no guessing

3. **Summary Agent Initialization:**
   - LLM: OpenAI gpt-4o-mini
   - Temperature: 0.2 (slightly creative for synthesis)
   - Purpose: Synthesize comprehensive answers
   - Features:
     - MCP tools enabled
     - Works with MapReduce query engine
     - Combines multiple chunks into coherent answer
   - System prompt: Synthesize from all relevant context

4. **Retriever Creation:**
   - Gets Needle Retriever (top_k=3, threshold=0.75)
   - Gets MapReduce Query Engine (for summaries)
   - Both configured by Index Layer

**Why Multiple Agents:** Different question types need different strategies. Atomic questions need precision, complex questions need comprehensiveness.

---

### **Stage 2.3: Orchestrator Initialization**

**Location:** `main.py` â†’ `orchestrator.py`

**What it does:**
Creates the central coordinator that manages the entire query pipeline.

**Detailed Process:**

1. **Dependency Injection:**
   - Receives all agents (router, needle, summary)
   - Receives all retrievers (needle_retriever, map_reduce_engine)
   - Stores references but doesn't create anything
   - Pure coordinator, no business logic

2. **Validation:**
   - Ensures all required components present
   - Validates at least one summary method available
   - Prints initialization summary

**Why Important:** Orchestrator is the single entry point. It coordinates all components but doesn't do the actual work. Clean separation of concerns.

---

### **Stage 2.4: Query Preprocessing**

**Location:** `orchestrator.py` â†’ `run()` method

**What it does:**
Analyzes the user's question to detect claim-specific queries that need filtering.

**Detailed Process:**

1. **Claim Number Extraction:**
   - Regex patterns:
     - "claim number 5" â†’ "5"
     - "claim #5" â†’ "5"
     - "form #5" â†’ "5"
     - "AUTO CLAIM FORM #5" â†’ "5"

2. **Claimant Name Extraction:**
   - Regex pattern: Capitalized first and last names
   - Examples:
     - "Jon Mor's phone" â†’ "Jon Mor"
     - "What is Jane Smith's address?" â†’ "Jane Smith"

3. **PostFilterRetriever Creation (if needed):**
   - If claim number or name detected:
     - Wraps base retriever
     - Retrieves 3x more results (e.g., 15 instead of 5)
     - Filters by metadata (claim_number OR claimant_name)
     - Returns top_k after filtering
   - Why needed: FAISS doesn't support native metadata filtering
   - Trade-off: Retrieve more, filter in Python

**Example:**
```
Question: "What is Jon Mor's phone number?"
â†’ Extracts: claimant_name = "Jon Mor"
â†’ Creates filtered retriever
â†’ Retrieval only searches Jon Mor's chunks
```

**Why Important:** Prevents cross-claim contamination. "Jon Mor's phone" should never return Jane Smith's phone number, even if semantically similar.

---

### **Stage 2.5: Routing Decision**

**Location:** `orchestrator.py` â†’ Router Agent

**What it does:**
Classifies the question to determine which agent should handle it.

**Detailed Process:**

1. **Router Agent Invocation:**
   - Sends question to Router Agent
   - Router LLM analyzes question intent
   - No retrieval at this stage (pure classification)

2. **Classification Logic:**
   - **NEEDLE Route:**
     - Single, specific fact needed
     - Examples: "What's the phone?", "When was the accident?"
     - Also: Date calculations ("How many days...?")
   
   - **SUMMARY Route:**
     - Multiple facts or explanation needed
     - Examples: "Summarize the claim", "What happened?"

3. **Output:**
   - route: "needle" or "summary"
   - confidence: 0.0 to 1.0
   - reason: Explanation of decision

**Example:**
```
Question: "What is Jon Mor's phone number?"
Route Decision:
  route: "needle"
  confidence: 0.95
  reason: "Question asks for single specific fact (phone number)"
```

**Why Important:** Different questions need different retrieval strategies. Routing ensures optimal retrieval for each question type.

---

### **Stage 2.6a: Needle Agent Execution (If Routed to NEEDLE)**

**Location:** `orchestrator.py` â†’ Needle Agent

**What it does:**
Retrieves precise chunks and extracts atomic facts.

**Detailed Process:**

1. **Retrieval:**
   - Uses Needle Retriever (top_k=3, threshold=0.75)
   - Embeds user question using OpenAI embedding
   - Performs cosine similarity search in FAISS
   - Returns 3 most similar chunks (if above threshold)
   - Each chunk is ~200 characters (child chunks)

2. **Context Preparation:**
   - Formats retrieved chunks for LLM
   - Includes chunk text + metadata
   - Adds claim context (claim_id, claimant_name)

3. **LLM Invocation (Standard Path):**
   - Sends question + chunks to Needle Agent LLM
   - System prompt: "Extract exact answer from chunks"
   - LLM reads chunks and extracts fact
   - Returns structured response (Pydantic model)

4. **MCP Tool Path (If Date Calculation Needed):**
   - LLM recognizes date calculation in question
   - Extracts dates from retrieved chunks
   - **Tool Call Decision:**
     - LLM with `tool_choice="auto"`
     - LLM decides: "I see two dates, I should call calculate_days_between"
   - **Tool Execution:**
     - Calls: `calculate_days_between("2024-01-24", "2024-02-18")`
     - Tool performs exact calculation using Python datetime
     - Returns: `{"success": True, "number_of_days": 25}`
   - **Final Answer Formation:**
     - LLM receives tool result
     - Formats natural language answer
     - Example: "25 days passed between the accident and repair appointment."

5. **Response Formation:**
   - answer: Extracted fact or "null" if not found
   - confidence: 1.0 if found, 0.0 if not
   - sources: List of chunk IDs used
   - reason: Explanation (may mention MCP tool usage)

**Example (Standard):**
```
Question: "What is Jon Mor's phone?"
Retrieved Chunks:
  1. "Name: Jon Mor, Phone: 555-1234"
  2. "Contact: Jon Mor, 555-1234"
  3. "Address: 123 Main St"

LLM Analysis: "Phone number is 555-1234, found in chunks 1 and 2"

Response:
  answer: "555-1234"
  confidence: 1.0
  sources: ["chunk_abc123", "chunk_def456"]
  reason: "Found exact phone number in contact section"
```

**Example (MCP Tool):**
```
Question: "How many days from accident to repair?"
Retrieved Chunks:
  1. "Accident Date: 2024-01-24"
  2. "Repair Appointment: 2024-02-18"
  3. "Claim filed on 2024-01-26"

LLM Analysis: 
  "I see two dates: 2024-01-24 and 2024-02-18"
  "I need to calculate days between them"
  "I should call the MCP tool"

Tool Call:
  calculate_days_between("2024-01-24", "2024-02-18")
  â†’ Returns: 25 days

Response:
  answer: "25 days"
  confidence: 1.0
  sources: ["chunk_xyz789", "chunk_abc123"]
  reason: "Used MCP date_calculator tool: calculate_days_between(2024-01-24, 2024-02-18) = 25 days"
```

**Why Needle Agent:** For atomic questions, you need high precision. Small chunks + high threshold + fact extraction = accurate answers without hallucination.

---

### **Stage 2.6b: Summary Agent Execution (If Routed to SUMMARY)**

**Location:** `orchestrator.py` â†’ Summary Agent

**What it does:**
Retrieves comprehensive context and synthesizes detailed answers.

**Detailed Process:**

1. **MapReduce Query Engine Approach (Preferred):**
   
   a. **Retrieval Phase:**
      - Uses MapReduce Query Engine
      - Retrieves top_k=15 chunks (more comprehensive)
      - Uses both parent and child chunks
      - No similarity threshold (high recall)
   
   b. **Map Phase:**
      - For each retrieved chunk:
        - LLM generates summary of that chunk
        - Focuses on question-relevant information
      - Results in 15 mini-summaries
   
   c. **Reduce Phase:**
      - LLM combines all mini-summaries
      - Synthesizes coherent final answer
      - Resolves any contradictions
      - Organizes information logically
   
   d. **Final Answer Formation:**
      - Comprehensive response covering all relevant aspects
      - May call MCP tool if date calculations involved

2. **MCP Tool Integration (If Needed):**
   - During synthesis, LLM may recognize need for date calculation
   - Calls calculate_days_between with dates from context
   - Incorporates exact calculation in final answer

3. **Response Formation:**
   - answer: Comprehensive synthesized response
   - confidence: 0.8-0.9 typically (synthesis less certain than extraction)
   - sources: All chunk IDs used (may be 15+)
   - reason: Explanation of synthesis process

**Example:**
```
Question: "Summarize Jon Mor's claim"

Retrieved Chunks (15 chunks covering):
  â€¢ Claimant info
  â€¢ Incident details
  â€¢ Vehicle damage
  â€¢ Repair information
  â€¢ Claim status
  â€¢ Payment details

Map Phase (15 mini-summaries):
  1. "Jon Mor, phone 555-1234..."
  2. "Accident on 2024-01-24 at Main St..."
  3. "Vehicle front bumper damaged..."
  ...

Reduce Phase:
  Combines all summaries into coherent narrative

Response:
  answer: "Jon Mor filed an insurance claim on January 26, 2024, 
           following a vehicle accident on January 24, 2024 at Main 
           Street. The accident resulted in front bumper damage to 
           his vehicle. The repair appointment was scheduled for 
           February 18, 2024 (25 days after the accident). The claim 
           amount is $5,000 and the status is approved. The payment 
           was issued on February 25, 2024."
  confidence: 0.9
  sources: [15+ chunk IDs]
  reason: "Synthesized comprehensive summary from incident, repair, 
           and payment sections. Used MCP tool for date calculation."
```

**Why Summary Agent:** Complex questions need broad context. MapReduce ensures comprehensive coverage while maintaining coherence. LLM synthesis creates natural, flowing answers.

---

### **Stage 2.7: Response Normalization**

**Location:** `orchestrator.py` â†’ `run()` method

**What it does:**
Formats the agent's response into a standardized structure for external consumers.

**Detailed Process:**

1. **Unified Response Creation:**
   - Combines routing metadata + agent result
   - Adds route information (which agent was used)
   - Ensures consistent format regardless of agent

2. **Response Structure:**
   ```python
   {
       "route": "needle" or "summary",
       "answer": "555-1234",
       "confidence": 1.0,
       "sources": ["chunk_abc123", "chunk_def456"],
       "retrieved_chunks_content": ["Phone: 555-1234", ...],
       "reason": "Found in contact section"
   }
   ```

3. **Logging:**
   - Prints route decision
   - Prints final answer
   - Prints confidence score
   - Prints number of sources
   - Prints reasoning

**Why Important:** External systems (GUI, API, evaluation) need consistent interface. Normalization ensures predictable response structure.

---

### **Flow 2 Summary:**

```
User Question: "What is Jon Mor's phone?"
    â†“
[Load Index] (2 seconds)
    â†’ production_index/ loaded into memory
    â†“
[Initialize Agents]
    â†’ Router, Needle, Summary agents ready
    â†“
[Orchestrator]
    â†“
[Query Preprocessing]
    â†’ Detects: claimant_name = "Jon Mor"
    â†’ Creates filtered retriever
    â†“
[Router Agent]
    â†’ Classifies: route = "needle"
    â†“
[Needle Agent]
    â†’ Retrieves 3 chunks (from Jon Mor's claim only)
    â†’ Extracts: "555-1234"
    â†“
[Response Normalization]
    â†’ Formats response
    â†“
Final Answer: "555-1234"
(Total time: ~2-3 seconds)
```

---

## **MCP Tools in the Flow**

### **What are MCP Tools?**

MCP (Model Context Protocol) Tools are **external deterministic functions** that extend LLM capabilities for precise computations that LLMs are bad at (like date arithmetic).

### **Available Tool: Date Calculator**

**Location:** `mcp_tools/date_calculator.py`

**Purpose:** Calculate exact number of days between two dates.

**Why Needed:**
- LLMs are bad at arithmetic (might say "approximately 25 days")
- LLMs can't reliably handle leap years
- Need exact, deterministic results
- No approximation or hallucination

### **How MCP Tools Work in Query Flow:**

```
User Query with Date Calculation:
"How many days from accident to repair?"
    â†“
[Router Agent]
    â†’ Classifies as NEEDLE (date calculation detected)
    â†“
[Needle Agent]
    â†’ Retrieves chunks:
       â€¢ "Accident: 2024-01-24"
       â€¢ "Repair: 2024-02-18"
    â†“
[LLM Analysis]
    â†’ Recognizes: "I see two dates, I need exact calculation"
    â†’ Decision: Call MCP tool
    â†“
[MCP Tool Invocation]
    â†’ Function: calculate_days_between("2024-01-24", "2024-02-18")
    â†’ Python datetime computation
    â†’ Returns: {"success": True, "number_of_days": 25}
    â†“
[LLM Response Formation]
    â†’ Receives: 25 days (exact)
    â†’ Formats: "25 days passed between accident and repair"
    â†“
Final Answer: "25 days" (deterministic, guaranteed correct)
```

### **Key Principles:**

1. **LLMs Orchestrate, Tools Compute:**
   - LLM understands question intent
   - LLM extracts dates from context
   - Tool performs exact calculation
   - LLM formats final answer

2. **Deterministic Computation:**
   - Same dates â†’ Same result, always
   - No approximation, no hallucination
   - Handles leap years, month boundaries

3. **Transparent Usage:**
   - Agent's "reason" field mentions tool usage
   - Example: "Used MCP date_calculator tool: ..."
   - Enables auditing and debugging

4. **Automatic Decision:**
   - LLM decides WHEN to use tool (tool_choice="auto")
   - No hardcoded rules in Python
   - LLM recognizes date calculation patterns

---

## **Evaluation Systems**

The project includes **two independent evaluation frameworks** to assess RAG system performance.

---

### **Evaluation 1: LLM-as-a-Judge (Primary)**

**Location:** `evaluation/`

**Purpose:** Custom evaluation framework tailored to insurance claim domain.

**Components:**

1. **Test Cases:**
   - File: `test_cases.json`
   - 8 test questions covering:
     - Atomic facts (phone, dates, amounts)
     - Complex questions (summaries, timelines)
     - Edge cases (unanswerable questions)
   - Each test case has:
     - question
     - ground_truth (expected answer)
     - expected_chunks (specific chunks that should be retrieved)

2. **Judge LLM:**
   - Model: Google Gemini 2.5-flash
   - Why different from RAG system (gpt-4o-mini):
     - Prevents bias (judge â‰  answerer)
     - Independent evaluation
     - Catches issues the answering LLM might miss

3. **Evaluation Metrics (3):**

   **a. Answer Correctness:**
   - Question: Does system's answer match ground truth?
   - Process:
     - Judge LLM compares system answer vs. ground truth
     - Semantic comparison (not exact string match)
     - Returns: 0.0 (wrong) to 1.0 (perfect)
   - Example:
     - Ground truth: "555-1234"
     - System answer: "The phone number is 555-1234"
     - Score: 1.0 (semantically equivalent)

   **b. Context Relevancy:**
   - Question: Are retrieved chunks relevant to question?
   - Process:
     - Judge LLM examines each retrieved chunk
     - Determines if chunk helps answer question
     - Calculates: (relevant chunks) / (total chunks)
   - Example:
     - Question: "What's the claim amount?"
     - Chunks: "Amount: $5,000" âœ…, "Phone: 555-1234" âŒ
     - Score: 0.5 (1 relevant, 1 irrelevant)

   **c. Context Recall (Expected Chunks):**
   - Question: Did we retrieve the expected chunks?
   - Process:
     - Test case specifies which chunks SHOULD be retrieved
     - Judge checks if expected chunks are in retrieved set
     - Calculates: (retrieved expected) / (total expected)
   - Example:
     - Expected chunks: ["chunk_123", "chunk_456"]
     - Retrieved chunks: ["chunk_123", "chunk_789"]
     - Score: 0.5 (found 1 of 2 expected)

4. **Evaluation Process:**
   - Run: `python evaluation/run_evaluation.py`
   - For each test case:
     - Query RAG system
     - Collect answer and retrieved chunks
     - Judge LLM evaluates all 3 metrics
     - Save scores
   - Output: `evaluation_results.json`

5. **Why Custom Evaluation:**
   - Domain-specific (knows insurance claims)
   - Expected chunks validation (structural check)
   - Tailored to project needs
   - Can add custom metrics easily

---

### **Evaluation 2: RAGAS (Secondary)**

**Location:** `evaluation-ragas/`

**Purpose:** Industry-standard RAG evaluation using established framework.

**Components:**

1. **Test Cases:**
   - Uses same `test_cases.json` from custom evaluation
   - Ensures consistency across evaluations

2. **Evaluator LLM:**
   - Model: OpenAI gpt-4o-mini
   - Why different from custom judge (Gemini):
     - Cross-validation with different LLM
     - More stable than Gemini experimental models
     - Fast and cost-effective

3. **RAGAS Metrics (4):**

   **a. Context Recall:**
   - Question: Can ground truth be attributed to retrieved contexts?
   - Process:
     - LLM checks if ground truth information exists in chunks
     - Different from custom "expected chunks" metric
     - Focuses on information content, not specific chunks
   - Example:
     - Ground truth: "555-1234"
     - Chunks: "Phone: 555-1234" âœ…
     - Score: 1.0

   **b. Context Precision:**
   - Question: Are relevant contexts ranked higher than irrelevant?
   - Process:
     - LLM evaluates each chunk for relevance
     - Calculates precision at each position
     - Higher scores = relevant chunks appear first
   - Example:
     - Position 1: Relevant âœ…
     - Position 2: Relevant âœ…
     - Position 3: Irrelevant âŒ
     - Score: 0.89 (weighted precision)

   **c. Faithfulness:**
   - Question: Is answer grounded in context? (No hallucination?)
   - Process:
     - LLM breaks answer into claims
     - For each claim, checks if supported by chunks
     - Calculates: (supported claims) / (total claims)
   - Example:
     - Answer: "Accident on Jan 24, driver was speeding"
     - Chunks: Only "Accident: Jan 24" âœ…, no speeding info âŒ
     - Score: 0.5 (1 supported, 1 hallucinated)

   **d. Answer Relevancy:**
   - Question: Does answer address the user's question?
   - Process:
     - LLM generates questions from the answer
     - Compares generated questions to original
     - High similarity = answer is relevant
   - Example:
     - Original Q: "What's the phone?"
     - Generated Q: "What is the phone number?" âœ…
     - Score: 0.95 (highly similar)

4. **Evaluation Process:**
   - Run: `python evaluation-ragas/ragas_eval.py`
   - For each test case:
     - Query RAG system
     - Collect answer and contexts
     - Build RAGAS dataset
     - RAGAS library evaluates all 4 metrics
     - Save scores
   - Output: `ragas_results.json`

5. **Visualization:**
   - Run: `python evaluation-ragas/visualize_results.py`
   - Generates: `ragas_visualization.png`
   - Charts:
     - Overall metric scores (bar chart)
     - Context precision by question
     - Answer relevancy by question
     - Heatmap of all metrics

6. **Why RAGAS:**
   - Industry-standard (comparable to other RAG systems)
   - Framework-agnostic (works with any RAG)
   - Comprehensive (4 metrics covering different aspects)
   - Well-maintained library

---

### **Evaluation Comparison:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       CUSTOM LLM-AS-A-JUDGE vs. RAGAS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  CUSTOM (evaluation/):                                  â”‚
â”‚  â€¢ Judge: Gemini 2.5-flash                              â”‚
â”‚  â€¢ Metrics: 3 (Answer, Context Relevancy, Expected)     â”‚
â”‚  â€¢ Focus: Domain-specific, structural validation        â”‚
â”‚  â€¢ Strength: Tailored to insurance claims               â”‚
â”‚                                                         â”‚
â”‚  RAGAS (evaluation-ragas/):                             â”‚
â”‚  â€¢ Judge: OpenAI gpt-4o-mini                            â”‚
â”‚  â€¢ Metrics: 4 (Recall, Precision, Faithfulness, Relevancy)â”‚
â”‚  â€¢ Focus: General RAG quality, standard framework       â”‚
â”‚  â€¢ Strength: Industry benchmarking, cross-validation    â”‚
â”‚                                                         â”‚
â”‚  WHY BOTH:                                              â”‚
â”‚  âœ… Cross-validation (different LLMs, different angles) â”‚
â”‚  âœ… Comprehensive coverage (7 total metrics)            â”‚
â”‚  âœ… Domain + General perspectives                       â”‚
â”‚  âœ… Increased confidence in results                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Running Evaluations:**

**Via Command Line:**
```bash
# Custom LLM-as-a-Judge
python evaluation/run_evaluation.py

# RAGAS
python evaluation-ragas/ragas_eval.py
python evaluation-ragas/visualize_results.py
```

**Via GUI:**
```bash
streamlit run app/gui_app.py

# In GUI:
# 1. Click "Run Custom Evaluation" button
# 2. Click "Run RAGAS Evaluation" button
# 3. Click "Compare Evaluations" to see side-by-side
```

---

### **Evaluation Results Interpretation:**

**Score Thresholds:**
- ğŸŸ¢ **Excellent (0.9-1.0):** System performing very well
- ğŸŸ¡ **Good (0.7-0.9):** Room for improvement, but functional
- ğŸŸ  **Moderate (0.5-0.7):** Significant issues, needs attention
- ğŸ”´ **Poor (0.0-0.5):** Critical problems, requires fixes

**Example Results:**
```
Custom LLM-as-a-Judge:
  Answer Correctness:      0.95 ğŸŸ¢
  Context Relevancy:       0.88 ğŸŸ¡
  Context Recall:          0.92 ğŸŸ¢

RAGAS:
  Context Recall:          0.95 ğŸŸ¢
  Context Precision:       0.87 ğŸŸ¡
  Faithfulness:            0.99 ğŸŸ¢
  Answer Relevancy:        0.92 ğŸŸ¢

Analysis:
  âœ… Excellent answer quality (correctness, relevancy, faithfulness)
  âœ… Good retrieval (high recall)
  âš ï¸  Context precision could improve (some irrelevant chunks)
  
Recommendation:
  Increase similarity_threshold from 0.75 to 0.80 to improve precision
```

---

# ğŸ“ **Short Version**

## **What is RagAgentv2?**

A production RAG system for insurance claim processing. Users ask questions about claim documents, system provides accurate, grounded answers.

---

## **Two Main Flows:**

### **Flow 1: Build Index (Once)**

**Purpose:** Transform PDF â†’ Searchable Index

**Steps:**
1. **PDF Ingestion:** PDF â†’ Clean text
2. **Claim Segmentation:** 1 PDF â†’ 20 claims
3. **Chunking:** Claims â†’ 550 hierarchical chunks
4. **Indexing:** Chunks â†’ Embedded vectors + FAISS

**Output:** `production_index/` folder

**Duration:** ~5-10 minutes

---

### **Flow 2: Answer Questions (Every Query)**

**Purpose:** Use index to answer questions fast

**Steps:**
1. **Load Index:** Load production_index/ (~2 sec)
2. **Initialize Agents:** Router, Needle, Summary
3. **Preprocess:** Detect claim-specific queries
4. **Route:** Router classifies question (needle/summary)
5. **Execute:** 
   - Needle Agent: Extract atomic facts (3 chunks, high precision)
   - Summary Agent: Synthesize comprehensive answers (15+ chunks, MapReduce)
   - MCP Tools: Call date calculator if needed (deterministic computation)
6. **Return:** Natural language answer

**Duration:** 2-5 seconds

---

## **MCP Tools:**

**What:** External functions for precise computation

**When:** Date calculations (e.g., "How many days from accident to repair?")

**How:** 
- LLM recognizes need for calculation
- Calls `calculate_days_between(start, end)`
- Tool returns exact days (no hallucination)
- LLM formats answer

---

## **Evaluation (Two Systems):**

### **1. Custom LLM-as-a-Judge:**
- Judge: Gemini 2.5-flash
- Metrics: Answer Correctness, Context Relevancy, Context Recall
- Focus: Insurance claims domain

### **2. RAGAS:**
- Judge: OpenAI gpt-4o-mini
- Metrics: Context Recall, Context Precision, Faithfulness, Answer Relevancy
- Focus: Industry-standard RAG evaluation

**Why Both:** Cross-validation, comprehensive coverage, different perspectives

---

# ğŸ¯ **Overall Project Summary**

## **Project Goal:**

Build a production-ready RAG system that enables natural language querying of insurance claim documents with high accuracy, no hallucination, and fast response times.

---

## **Key Features:**

1. **Multi-Claim Processing:**
   - Handles PDFs with 20+ claims
   - Claim-level isolation prevents cross-contamination

2. **Hierarchical Chunking:**
   - 3-level structure (sections, parents, children)
   - Optimized for different question types

3. **Intelligent Routing:**
   - Automatic classification (needle vs. summary)
   - Different strategies for different questions

4. **MCP Tool Integration:**
   - Deterministic date calculations
   - No approximation or hallucination
   - Transparent tool usage

5. **Dual Evaluation:**
   - Custom + RAGAS frameworks
   - Comprehensive quality assessment
   - Cross-validation

6. **Production-Ready:**
   - Two-phase architecture (build once, query many)
   - Fast response times (2-5 seconds)
   - Scalable (handles concurrent users)

---

## **Technology Stack:**

- **RAG Framework:** LlamaIndex
- **LLM Orchestration:** LangChain
- **Vector Database:** FAISS
- **Embeddings:** OpenAI text-embedding-3-small
- **LLMs:** OpenAI gpt-4o-mini (RAG), Gemini 2.5-flash (evaluation)
- **PDF Processing:** pypdf
- **Evaluation:** Custom + RAGAS
- **GUI:** Streamlit

---

## **System Components:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SYSTEM ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  BUILD TIME (Flow 1):                                   â”‚
â”‚  PDF Ingestion â†’ Claim Segmentation â†’ Chunking â†’ Index â”‚
â”‚                                                         â”‚
â”‚  QUERY TIME (Flow 2):                                   â”‚
â”‚  Load Index â†’ Agents â†’ Orchestrator â†’ Route â†’ Answer   â”‚
â”‚                                                         â”‚
â”‚  AGENTS:                                                â”‚
â”‚  Router (classify) â†’ Needle (extract) â†’ Summary (synthesize)â”‚
â”‚                                                         â”‚
â”‚  EXTENSIONS:                                            â”‚
â”‚  MCP Tools (date calculations)                          â”‚
â”‚                                                         â”‚
â”‚  EVALUATION:                                            â”‚
â”‚  Custom LLM-as-a-Judge + RAGAS (7 total metrics)        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **Success Metrics:**

Based on current evaluation results:
- âœ… Answer Correctness: 0.95 (Excellent)
- âœ… Faithfulness: 0.99 (No hallucination)
- âœ… Context Recall: 0.95 (Retrieving right information)
- ğŸŸ¡ Context Precision: 0.87 (Good, room for optimization)

---

## **Use Cases:**

1. **Atomic Queries:** "What is Jon Mor's phone number?" â†’ "555-1234"
2. **Complex Queries:** "Summarize the claim" â†’ Comprehensive timeline
3. **Date Calculations:** "Days from accident to repair?" â†’ "25 days" (MCP tool)
4. **Claim-Specific:** "Jon Mor's phone?" â†’ Only Jon's data (no cross-contamination)
5. **Unanswerable:** "What's the color?" â†’ "null" (honest, no guessing)

---

## **Key Design Principles:**

1. **Separation of Concerns:** Each layer has one job
2. **Build Once, Query Many:** Efficiency through pre-computation
3. **No Hallucination:** Strict grounding, MCP tools for computation
4. **Claim Isolation:** Independent processing per claim
5. **Hierarchical Retrieval:** Different granularity for different questions
6. **Deterministic Tools:** LLMs orchestrate, tools compute
7. **Dual Evaluation:** Multiple perspectives for comprehensive assessment

---

## **Project Structure:**

```
RagAgentv2/
â”œâ”€â”€ RAG/
â”‚   â”œâ”€â”€ PDF_Ingestion/          (Stage 1.1: PDF â†’ Clean text)
â”‚   â”œâ”€â”€ Claim_Segmentation/     (Stage 1.2: Split claims)
â”‚   â”œâ”€â”€ Chunking_Layer/         (Stage 1.3: Create hierarchy)
â”‚   â”œâ”€â”€ Index_Layer/            (Stage 1.4: Embed + FAISS)
â”‚   â”œâ”€â”€ Agents/                 (Router, Needle, Summary)
â”‚   â””â”€â”€ Orchestration/          (Coordinator)
â”‚
â”œâ”€â”€ mcp_tools/                  (Date calculator tool)
â”‚
â”œâ”€â”€ evaluation/                 (Custom LLM-as-a-Judge)
â”œâ”€â”€ evaluation-ragas/           (RAGAS evaluation)
â”‚
â”œâ”€â”€ app/                        (Streamlit GUI)
â”‚
â”œâ”€â”€ build_production_index.py   (Flow 1: Build index)
â”œâ”€â”€ main.py                     (Flow 2: Query system)
â”‚
â””â”€â”€ production_index/           (Pre-built index)
```

---

## **Quick Start:**

```bash
# 1. Build index (once)
python build_production_index.py

# 2. Query system (interactive)
python main.py

# 3. Or use GUI
streamlit run app/gui_app.py

# 4. Run evaluations
python evaluation/run_evaluation.py
python evaluation-ragas/ragas_eval.py
```

---

## **Future Enhancements:**

- Add more MCP tools (currency conversion, unit conversion)
- Support for more document types (emails, forms)
- Real-time index updates (incremental indexing)
- Multi-document queries across claims
- Advanced analytics dashboard
- API deployment for external systems

---

**Built for production-grade insurance claim processing with accuracy, speed, and reliability.** ğŸš—ğŸ“„ğŸ¤–

---

## **Documentation Map:**

For deeper understanding of each component, see:

- `RAG/PDF_Ingestion/pdf-ingestion-explained.md`
- `RAG/Claim_Segmentation/claim-segmentation-explained.md`
- `RAG/Chunking_Layer/chunking-layer-explained.md`
- `RAG/Index_Layer/index-layer-explained.md`
- `RAG/Agents/agents-explained.md`
- `RAG/Orchestration/orchestrator-explained.md`
- `mcp_tools/mcp-tools-explained.md`
- `evaluation/evaluation_explained.md`
- `evaluation-ragas/evaluation-ragas-explained.md`
- `RAG_SYSTEM_FLOW.md`

**This document (PROJECT_OVERVIEW.md) provides the complete picture of how everything fits together.** ğŸ¯
