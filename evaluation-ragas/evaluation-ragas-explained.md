# RAGAS Evaluation - Complete Guide

## ğŸ“Š **What is RAGAS?**

**RAGAS** (Retrieval-Augmented Generation Assessment) is a **framework-agnostic evaluation library** specifically designed to evaluate RAG (Retrieval-Augmented Generation) systems. It provides automated, LLM-based metrics to assess both retrieval quality and generation quality.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                RAGAS EVALUATION                         â”‚
â”‚       (Secondary Evaluation Framework)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  PURPOSE:                                               â”‚
â”‚  Automated evaluation of RAG system performance         â”‚
â”‚                                                         â”‚
â”‚  WHAT IT EVALUATES:                                     â”‚
â”‚  â€¢ Retrieval quality (precision, recall)                â”‚
â”‚  â€¢ Generation quality (faithfulness, relevancy)         â”‚
â”‚                                                         â”‚
â”‚  HOW:                                                   â”‚
â”‚  â€¢ Uses LLM (gpt-4o-mini) as evaluator                  â”‚
â”‚  â€¢ Compares system outputs against ground truth         â”‚
â”‚  â€¢ Provides automated scores (0.0 to 1.0)               â”‚
â”‚                                                         â”‚
â”‚  4 KEY METRICS:                                         â”‚
â”‚  1. Context Recall    (Did we retrieve ground truth?)   â”‚
â”‚  2. Context Precision (Are retrieved chunks relevant?)  â”‚
â”‚  3. Faithfulness      (Is answer grounded in context?)  â”‚
â”‚  4. Answer Relevancy  (Does answer address question?)   â”‚
â”‚                                                         â”‚
â”‚  CRITICAL:                                              â”‚
â”‚  âŒ NOT used during inference                           â”‚
â”‚  âŒ NOT a replacement for primary evaluation            â”‚
â”‚  âœ… Complements LLM-as-a-Judge evaluation               â”‚
â”‚  âœ… Provides additional insights                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ **Why RAGAS?**

### **The Need for Automated RAG Evaluation:**

```
RAG SYSTEM CHALLENGES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Traditional metrics (BLEU, ROUGE) don't work well for RAG:
  âŒ Can't evaluate retrieval quality
  âŒ Can't measure faithfulness (hallucination)
  âŒ Can't assess context relevance
  âŒ Don't understand semantic similarity

RAGAS solves this by:
  âœ… Using LLM as intelligent evaluator
  âœ… Evaluating both retrieval and generation
  âœ… Providing multiple complementary metrics
  âœ… Being framework-agnostic (works with any RAG system)
```

---

### **RAGAS vs. LLM-as-a-Judge:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       TWO EVALUATION APPROACHES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  LLM-AS-A-JUDGE (Primary Evaluation):                  â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ Custom evaluation logic                              â”‚
â”‚  â€¢ Gemini 2.5-flash as judge                            â”‚
â”‚  â€¢ 3 metrics:                                           â”‚
â”‚    - Answer Correctness                                 â”‚
â”‚    - Context Relevancy                                  â”‚
â”‚    - Context Recall (expected chunks)                   â”‚
â”‚  â€¢ Tailored to our claims system                        â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  RAGAS (Secondary Evaluation):                          â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ Standard evaluation framework                        â”‚
â”‚  â€¢ OpenAI gpt-4o-mini as evaluator                      â”‚
â”‚  â€¢ 4 metrics:                                           â”‚
â”‚    - Context Recall (ground truth attribution)          â”‚
â”‚    - Context Precision (ranking quality)                â”‚
â”‚    - Faithfulness (grounding)                           â”‚
â”‚    - Answer Relevancy (question alignment)              â”‚
â”‚  â€¢ Industry-standard, comparable                        â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  WHY BOTH?                                              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  âœ… Different perspectives on system quality            â”‚
â”‚  âœ… Validate findings across frameworks                 â”‚
â”‚  âœ… Comprehensive evaluation coverage                   â”‚
â”‚  âœ… Industry-standard benchmarking (RAGAS)              â”‚
â”‚  âœ… Custom domain-specific checks (LLM-as-a-Judge)      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ **RAGAS Metrics Explained**

### **Metric 1: Context Recall**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CONTEXT RECALL                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  QUESTION:                                              â”‚
â”‚  Can the ground truth be attributed to the              â”‚
â”‚  retrieved contexts?                                    â”‚
â”‚                                                         â”‚
â”‚  FORMULA:                                               â”‚
â”‚  Recall = (GT sentences in contexts) / (Total GT sentences)â”‚
â”‚                                                         â”‚
â”‚  EXAMPLE:                                               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Question: "What is Jon Mor's phone number?"            â”‚
â”‚                                                         â”‚
â”‚  Ground Truth: "555-1234"                               â”‚
â”‚                                                         â”‚
â”‚  Retrieved Contexts:                                    â”‚
â”‚    â€¢ Chunk 1: "Name: Jon Mor, Phone: 555-1234"         â”‚
â”‚    â€¢ Chunk 2: "Address: 123 Main St"                   â”‚
â”‚    â€¢ Chunk 3: "Accident date: 2024-01-24"              â”‚
â”‚                                                         â”‚
â”‚  Analysis:                                              â”‚
â”‚  LLM checks: Can "555-1234" be found in contexts?       â”‚
â”‚  Answer: YES (in Chunk 1)                               â”‚
â”‚  Score: 1.0 âœ…                                          â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  WHAT IT MEASURES:                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ Retrieval completeness                               â”‚
â”‚  â€¢ Did we fetch the right information?                  â”‚
â”‚  â€¢ Are we missing key facts?                            â”‚
â”‚                                                         â”‚
â”‚  INTERPRETATION:                                        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ 1.0: Perfect - all ground truth found                â”‚
â”‚  â€¢ 0.8: Good - most ground truth found                  â”‚
â”‚  â€¢ 0.5: Moderate - half of ground truth missing         â”‚
â”‚  â€¢ 0.0: Poor - ground truth not in contexts             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Metric 2: Context Precision**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CONTEXT PRECISION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  QUESTION:                                              â”‚
â”‚  Are the relevant contexts ranked higher than           â”‚
â”‚  irrelevant ones?                                       â”‚
â”‚                                                         â”‚
â”‚  FORMULA:                                               â”‚
â”‚  Precision@k = (Relevant contexts in top-k) / k         â”‚
â”‚                                                         â”‚
â”‚  EXAMPLE:                                               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Question: "What is the claim amount?"                  â”‚
â”‚                                                         â”‚
â”‚  Ground Truth: "$5,000"                                 â”‚
â”‚                                                         â”‚
â”‚  Retrieved Contexts (in order):                         â”‚
â”‚    1. "Claim Amount: $5,000" âœ… RELEVANT                â”‚
â”‚    2. "Approved on 2024-02-18" âœ… RELEVANT              â”‚
â”‚    3. "Name: Jon Mor" âŒ NOT RELEVANT                   â”‚
â”‚                                                         â”‚
â”‚  Analysis:                                              â”‚
â”‚  LLM checks each chunk:                                 â”‚
â”‚  â€¢ Chunk 1: Relevant to question                        â”‚
â”‚  â€¢ Chunk 2: Relevant to question                        â”‚
â”‚  â€¢ Chunk 3: Not relevant to question                    â”‚
â”‚                                                         â”‚
â”‚  Precision calculation:                                 â”‚
â”‚  Top-1: 1/1 = 1.0                                       â”‚
â”‚  Top-2: 2/2 = 1.0                                       â”‚
â”‚  Top-3: 2/3 = 0.67                                      â”‚
â”‚  Average: ~0.89 âœ…                                      â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  WHAT IT MEASURES:                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ Retrieval accuracy                                   â”‚
â”‚  â€¢ Signal-to-noise ratio                                â”‚
â”‚  â€¢ Are we fetching irrelevant chunks?                   â”‚
â”‚                                                         â”‚
â”‚  INTERPRETATION:                                        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ 1.0: Perfect - all top chunks relevant               â”‚
â”‚  â€¢ 0.8: Good - most top chunks relevant                 â”‚
â”‚  â€¢ 0.5: Moderate - half of top chunks irrelevant        â”‚
â”‚  â€¢ 0.0: Poor - all chunks irrelevant                    â”‚
â”‚                                                         â”‚
â”‚  WHY IT MATTERS:                                        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Low precision â†’ Wasting tokens on irrelevant context   â”‚
â”‚  High precision â†’ Efficient, focused retrieval          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Metric 3: Faithfulness**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             FAITHFULNESS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  QUESTION:                                              â”‚
â”‚  Is the generated answer grounded in the                â”‚
â”‚  retrieved contexts? (No hallucination?)                â”‚
â”‚                                                         â”‚
â”‚  FORMULA:                                               â”‚
â”‚  Faithfulness = (Supported claims) / (Total claims)     â”‚
â”‚                                                         â”‚
â”‚  EXAMPLE:                                               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Question: "What happened in the accident?"             â”‚
â”‚                                                         â”‚
â”‚  Retrieved Contexts:                                    â”‚
â”‚    â€¢ "Accident on 2024-01-24 at Main St"               â”‚
â”‚    â€¢ "Vehicle damage: front bumper"                     â”‚
â”‚                                                         â”‚
â”‚  Generated Answer:                                      â”‚
â”‚  "The accident occurred on January 24, 2024 at         â”‚
â”‚   Main Street. The vehicle's front bumper was damaged." â”‚
â”‚                                                         â”‚
â”‚  Analysis:                                              â”‚
â”‚  LLM breaks answer into claims:                         â”‚
â”‚    1. "Accident on January 24, 2024" âœ… (in context)   â”‚
â”‚    2. "At Main Street" âœ… (in context)                 â”‚
â”‚    3. "Front bumper damaged" âœ… (in context)           â”‚
â”‚                                                         â”‚
â”‚  Faithfulness: 3/3 = 1.0 âœ… (Perfect!)                 â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  COUNTER-EXAMPLE (Hallucination):                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Generated Answer (BAD):                                â”‚
â”‚  "The accident occurred on January 24, 2024 at         â”‚
â”‚   Main Street. The driver was speeding and ran a       â”‚
â”‚   red light."                                           â”‚
â”‚                                                         â”‚
â”‚  Analysis:                                              â”‚
â”‚    1. "Accident on January 24, 2024" âœ… (in context)   â”‚
â”‚    2. "At Main Street" âœ… (in context)                 â”‚
â”‚    3. "Driver was speeding" âŒ (NOT in context!)       â”‚
â”‚    4. "Ran a red light" âŒ (NOT in context!)           â”‚
â”‚                                                         â”‚
â”‚  Faithfulness: 2/4 = 0.5 âŒ (Hallucination detected!)  â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  WHAT IT MEASURES:                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ Hallucination detection                              â”‚
â”‚  â€¢ Answer grounding                                     â”‚
â”‚  â€¢ Factual accuracy                                     â”‚
â”‚                                                         â”‚
â”‚  INTERPRETATION:                                        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ 1.0: Perfect - no hallucination                      â”‚
â”‚  â€¢ 0.8: Good - minor unsupported claims                 â”‚
â”‚  â€¢ 0.5: Moderate - significant hallucination            â”‚
â”‚  â€¢ 0.0: Poor - completely made up answer                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Metric 4: Answer Relevancy**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ANSWER RELEVANCY                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  QUESTION:                                              â”‚
â”‚  Does the answer address the user's question?           â”‚
â”‚  Is it relevant and complete?                           â”‚
â”‚                                                         â”‚
â”‚  HOW IT WORKS:                                          â”‚
â”‚  LLM generates questions from the answer,               â”‚
â”‚  then compares similarity to original question          â”‚
â”‚                                                         â”‚
â”‚  EXAMPLE:                                               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Original Question:                                     â”‚
â”‚  "What is Jon Mor's phone number?"                      â”‚
â”‚                                                         â”‚
â”‚  Generated Answer:                                      â”‚
â”‚  "Jon Mor's phone number is 555-1234."                  â”‚
â”‚                                                         â”‚
â”‚  Analysis:                                              â”‚
â”‚  LLM generates questions from answer:                   â”‚
â”‚    â€¢ "What is Jon Mor's phone number?" âœ…              â”‚
â”‚                                                         â”‚
â”‚  Similarity to original: Very high!                     â”‚
â”‚  Score: 0.95 âœ…                                         â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  COUNTER-EXAMPLE (Low Relevancy):                       â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  Original Question:                                     â”‚
â”‚  "What is Jon Mor's phone number?"                      â”‚
â”‚                                                         â”‚
â”‚  Generated Answer (BAD):                                â”‚
â”‚  "Jon Mor filed a claim on 2024-01-24 for a            â”‚
â”‚   vehicle accident. The claim was approved."            â”‚
â”‚                                                         â”‚
â”‚  Analysis:                                              â”‚
â”‚  LLM generates questions from answer:                   â”‚
â”‚    â€¢ "When did Jon Mor file a claim?" âŒ               â”‚
â”‚    â€¢ "Was the claim approved?" âŒ                       â”‚
â”‚                                                         â”‚
â”‚  Similarity to original: Very low!                      â”‚
â”‚  (Answer doesn't address phone number)                  â”‚
â”‚  Score: 0.2 âŒ                                          â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  WHAT IT MEASURES:                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ Answer completeness                                  â”‚
â”‚  â€¢ Question-answer alignment                            â”‚
â”‚  â€¢ Answer focus                                         â”‚
â”‚                                                         â”‚
â”‚  INTERPRETATION:                                        â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚  â€¢ 1.0: Perfect - directly answers question             â”‚
â”‚  â€¢ 0.8: Good - mostly answers question                  â”‚
â”‚  â€¢ 0.5: Moderate - partially answers                    â”‚
â”‚  â€¢ 0.0: Poor - doesn't address question                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ **RAGAS Evaluation Flow**

### **Complete Pipeline:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAGAS EVALUATION PIPELINE (7 STEPS)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Input: Test Cases (test_cases.json)                    â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 1: Load Test Cases                        â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ Load from: evaluation/test_cases.json          â”‚     â”‚
â”‚  â”‚ Each test case has:                            â”‚     â”‚
â”‚  â”‚   â€¢ question                                   â”‚     â”‚
â”‚  â”‚   â€¢ ground_truth                               â”‚     â”‚
â”‚  â”‚   â€¢ expected_chunks                            â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ Example:                                       â”‚     â”‚
â”‚  â”‚ {                                              â”‚     â”‚
â”‚  â”‚   "id": "q1",                                  â”‚     â”‚
â”‚  â”‚   "question": "What is Jon Mor's phone?",      â”‚     â”‚
â”‚  â”‚   "ground_truth": "555-1234"                   â”‚     â”‚
â”‚  â”‚ }                                              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 2: Initialize RAG System                  â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ â€¢ Load production index                        â”‚     â”‚
â”‚  â”‚ â€¢ Create retrievers (needle, summary)          â”‚     â”‚
â”‚  â”‚ â€¢ Initialize agents (router, needle, summary)  â”‚     â”‚
â”‚  â”‚ â€¢ Create orchestrator                          â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ WHY: Need working RAG system to evaluate       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 3: Query RAG System                       â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ For each test case:                            â”‚     â”‚
â”‚  â”‚   1. Send question to orchestrator             â”‚     â”‚
â”‚  â”‚   2. Collect answer                            â”‚     â”‚
â”‚  â”‚   3. Collect retrieved contexts (chunk texts)  â”‚     â”‚
â”‚  â”‚   4. Collect metadata                          â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ Result:                                        â”‚     â”‚
â”‚  â”‚   {                                            â”‚     â”‚
â”‚  â”‚     "answer": "555-1234",                      â”‚     â”‚
â”‚  â”‚     "contexts": ["Phone: 555-1234", ...],      â”‚     â”‚
â”‚  â”‚     "sources": ["chunk_123", ...]              â”‚     â”‚
â”‚  â”‚   }                                            â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 4: Build RAGAS Dataset                    â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ Combine into Hugging Face Dataset:             â”‚     â”‚
â”‚  â”‚ {                                              â”‚     â”‚
â”‚  â”‚   "question": [...],                           â”‚     â”‚
â”‚  â”‚   "answer": [...],                             â”‚     â”‚
â”‚  â”‚   "contexts": [...],  # List of chunk texts    â”‚     â”‚
â”‚  â”‚   "ground_truth": [...]                        â”‚     â”‚
â”‚  â”‚ }                                              â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ WHY: RAGAS requires this specific format       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 5: Initialize Evaluator LLM               â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ LLM: OpenAI gpt-4o-mini                        â”‚     â”‚
â”‚  â”‚ Temperature: 0.0 (deterministic)               â”‚     â”‚
â”‚  â”‚ Timeout: 60s                                   â”‚     â”‚
â”‚  â”‚ Max Retries: 3                                 â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ WHY gpt-4o-mini:                               â”‚     â”‚
â”‚  â”‚   â€¢ Fast and cost-effective                    â”‚     â”‚
â”‚  â”‚   â€¢ More stable than Gemini experimental       â”‚     â”‚
â”‚  â”‚   â€¢ Reliable for evaluation                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 6: Run RAGAS Evaluation                   â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ For each test case:                            â”‚     â”‚
â”‚  â”‚   For each metric:                             â”‚     â”‚
â”‚  â”‚     â€¢ LLM evaluates                            â”‚     â”‚
â”‚  â”‚     â€¢ Returns score (0.0 to 1.0)               â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ Metrics evaluated:                             â”‚     â”‚
â”‚  â”‚   1. context_recall                            â”‚     â”‚
â”‚  â”‚   2. context_precision                         â”‚     â”‚
â”‚  â”‚   3. faithfulness                              â”‚     â”‚
â”‚  â”‚   4. answer_relevancy                          â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ WHY: Each metric provides different insight    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ STEP 7: Save Results                           â”‚     â”‚
â”‚  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚     â”‚
â”‚  â”‚ Save to: ragas_results.json                    â”‚     â”‚
â”‚  â”‚                                                â”‚     â”‚
â”‚  â”‚ Format:                                        â”‚     â”‚
â”‚  â”‚ {                                              â”‚     â”‚
â”‚  â”‚   "results": [                                 â”‚     â”‚
â”‚  â”‚     {                                          â”‚     â”‚
â”‚  â”‚       "question_id": "q1",                     â”‚     â”‚
â”‚  â”‚       "context_recall": 1.0,                   â”‚     â”‚
â”‚  â”‚       "context_precision": 0.89,               â”‚     â”‚
â”‚  â”‚       "faithfulness": 1.0,                     â”‚     â”‚
â”‚  â”‚       "answer_relevancy": 0.95                 â”‚     â”‚
â”‚  â”‚     },                                         â”‚     â”‚
â”‚  â”‚     ...                                        â”‚     â”‚
â”‚  â”‚   ],                                           â”‚     â”‚
â”‚  â”‚   "summary": {                                 â”‚     â”‚
â”‚  â”‚     "avg_context_recall": 0.95,                â”‚     â”‚
â”‚  â”‚     "avg_context_precision": 0.87,             â”‚     â”‚
â”‚  â”‚     "avg_faithfulness": 0.99,                  â”‚     â”‚
â”‚  â”‚     "avg_answer_relevancy": 0.92               â”‚     â”‚
â”‚  â”‚   }                                            â”‚     â”‚
â”‚  â”‚ }                                              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â†“                                                     â”‚
â”‚  Output: ragas_results.json + Summary printed           â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ **Key Concepts**

### **1. LLM as Evaluator**

```
WHY USE LLM FOR EVALUATION?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Traditional metrics (BLEU, ROUGE):
  âŒ Can't understand semantic similarity
  âŒ Can't detect hallucination
  âŒ Can't evaluate context relevance
  âŒ Too rigid for RAG evaluation

LLM as evaluator (RAGAS approach):
  âœ… Understands semantic meaning
  âœ… Can reason about relevance
  âœ… Can detect hallucination
  âœ… Flexible, human-like judgment

EXAMPLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Question: "What is Jon Mor's phone?"
Ground Truth: "555-1234"
System Answer: "The phone number is 555-1234"

BLEU Score: Low (different wording)
LLM Evaluation: High (same meaning)
```

---

### **2. Framework-Agnostic**

```
RAGAS WORKS WITH ANY RAG SYSTEM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Only needs 4 inputs:
  1. question
  2. answer (generated by RAG system)
  3. contexts (retrieved chunks)
  4. ground_truth (expected answer)

Works with:
  âœ… LlamaIndex
  âœ… LangChain
  âœ… Haystack
  âœ… Custom RAG systems
  âœ… Any retrieval + generation pipeline

WHY: Standard evaluation across tools
```

---

### **3. Offline Analysis Only**

```
RAGAS IS NOT USED DURING INFERENCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INFERENCE TIME (Real queries)     â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ User â†’ RAG System â†’ Answer         â”‚
â”‚ (No RAGAS involved)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EVALUATION TIME (Test suite)      â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ Test Cases â†’ RAG System â†’ Answers  â”‚
â”‚            â†“                       â”‚
â”‚         RAGAS Evaluation           â”‚
â”‚            â†“                       â”‚
â”‚         Scores + Insights          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WHY OFFLINE:
  âœ… No latency impact on users
  âœ… Detailed analysis without time pressure
  âœ… Batch evaluation of test suite
  âœ… Comprehensive metrics
```

---

### **4. Complementary to LLM-as-a-Judge**

```
TWO EVALUATION FRAMEWORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LLM-as-a-Judge:
  â€¢ Custom evaluation logic
  â€¢ Domain-specific (insurance claims)
  â€¢ Gemini 2.5-flash
  â€¢ Expected chunks validation

RAGAS:
  â€¢ Standard evaluation framework
  â€¢ General RAG evaluation
  â€¢ OpenAI gpt-4o-mini
  â€¢ Ground truth attribution

TOGETHER:
  âœ… Cross-validate findings
  âœ… Multiple perspectives
  âœ… Comprehensive coverage
  âœ… Industry-standard + custom
```

---

## ğŸ“Š **Usage Examples**

### **Running RAGAS Evaluation:**

```bash
# From project root
cd evaluation-ragas

# Install dependencies
pip install -r requirements.txt

# Set API key
export OPENAI_API_KEY="your-openai-key"

# Run evaluation
python ragas_eval.py

# Expected output:
# ================================================================
# ğŸ”‘ Checking API Keys...
# ================================================================
# âœ… OPENAI_API_KEY found
#
# ğŸ”§ Initializing RAG system...
# ...
# ğŸ“Š Building RAGAS dataset from 8 test cases...
# ...
# ğŸ”¬ Running RAGAS evaluation...
# ...
# âœ… Results saved to: ragas_results.json
#
# ======================================================================
# ğŸ“Š RAGAS EVALUATION SUMMARY
# ======================================================================
# Total test cases: 8
#
# Average Scores:
#   context_recall: 0.950
#   context_precision: 0.870
#   faithfulness: 0.990
#   answer_relevancy: 0.920
# ======================================================================
```

---

### **Visualizing Results:**

```bash
# Generate visualization
python visualize_results.py

# Creates: ragas_visualization.png
# Shows:
# - Overall metric scores (bar chart)
# - Context precision by question (bar chart)
# - Answer relevancy by question (bar chart)
# - Heatmap of all metrics by question
```

---

### **Viewing Results in GUI:**

```bash
# From project root
streamlit run app/gui_app.py

# In GUI:
# 1. Click "Run RAGAS Evaluation"
# 2. Wait for completion
# 3. Click "Show RAGAS Charts"
# 4. Click "Compare Evaluations" (LLM-as-a-Judge vs. RAGAS)
```

---

## ğŸ” **Interpreting Results**

### **Score Thresholds:**

```
METRIC SCORE INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸŸ¢ Excellent: 0.9 - 1.0
  â€¢ System performing very well
  â€¢ Minor improvements only

ğŸŸ¡ Good: 0.7 - 0.9
  â€¢ System performing adequately
  â€¢ Room for improvement

ğŸŸ  Moderate: 0.5 - 0.7
  â€¢ System has significant issues
  â€¢ Needs attention

ğŸ”´ Poor: 0.0 - 0.5
  â€¢ System failing on this metric
  â€¢ Critical issues to fix
```

---

### **Example Analysis:**

```
SAMPLE RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
context_recall: 0.950 ğŸŸ¢ Excellent
context_precision: 0.870 ğŸŸ¡ Good
faithfulness: 0.990 ğŸŸ¢ Excellent
answer_relevancy: 0.920 ğŸŸ¢ Excellent


INTERPRETATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… STRENGTHS:
  â€¢ High context_recall (0.95)
    â†’ Retrieving ground truth effectively
    â†’ Chunking and indexing working well

  â€¢ Excellent faithfulness (0.99)
    â†’ No hallucination
    â†’ Answers grounded in context
    â†’ Agents following instructions

  â€¢ High answer_relevancy (0.92)
    â†’ Answers addressing questions
    â†’ Good answer formatting


âš ï¸  AREAS FOR IMPROVEMENT:
  â€¢ Context_precision (0.87)
    â†’ Some irrelevant chunks retrieved
    â†’ Can improve similarity_threshold
    â†’ Can tune top_k parameter

RECOMMENDATIONS:
  1. Increase similarity_threshold from 0.75 to 0.80
  2. Monitor precision vs. recall trade-off
  3. Test with adjusted parameters
```

---

## âš™ï¸ **Configuration**

### **Retriever Settings:**

```python
# In ragas_eval.py:

# Needle Retriever (for atomic facts)
needle_retriever = index_manager.get_needle_retriever(
    top_k=3,              # Fewer chunks (precision)
    similarity_threshold=0.75,  # Higher threshold (quality)
)

# MapReduce Query Engine (for comprehensive answers)
map_reduce_query_engine = index_manager.get_map_reduce_query_engine(
    top_k=15,  # More chunks (recall)
)

# WHY THESE SETTINGS:
# - Optimized from evaluation results
# - Balances precision and recall
# - Different strategies for different question types
```

---

### **Evaluation LLM:**

```python
# In ragas_eval.py:

self.llm = ChatOpenAI(
    model="gpt-4o-mini",  # Fast, cost-effective
    api_key=api_key,
    temperature=0.0,      # Deterministic evaluation
    timeout=60,           # 60s timeout per call
    max_retries=3,        # Retry on failures
)

# WHY gpt-4o-mini:
# âœ… More stable than Gemini experimental models
# âœ… Fast and cost-effective
# âœ… Reliable for evaluation
# âœ… Good performance on evaluation tasks
```

---

## âœ… **Summary: RAGAS Evaluation**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RAGAS EVALUATION SUMMARY                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  WHAT:                                                  â”‚
â”‚  Framework-agnostic RAG evaluation library              â”‚
â”‚                                                         â”‚
â”‚  PURPOSE:                                               â”‚
â”‚  Automated, LLM-based evaluation of RAG systems         â”‚
â”‚                                                         â”‚
â”‚  4 KEY METRICS:                                         â”‚
â”‚  1. Context Recall    (Retrieval completeness)          â”‚
â”‚  2. Context Precision (Retrieval accuracy)              â”‚
â”‚  3. Faithfulness      (No hallucination)                â”‚
â”‚  4. Answer Relevancy  (Question alignment)              â”‚
â”‚                                                         â”‚
â”‚  HOW IT WORKS:                                          â”‚
â”‚  1. Load test cases                                     â”‚
â”‚  2. Query RAG system                                    â”‚
â”‚  3. Collect outputs (answer, contexts)                  â”‚
â”‚  4. Build RAGAS dataset                                 â”‚
â”‚  5. Run evaluation (gpt-4o-mini as judge)               â”‚
â”‚  6. Save results to JSON                                â”‚
â”‚                                                         â”‚
â”‚  EVALUATOR LLM:                                         â”‚
â”‚  OpenAI gpt-4o-mini (different from RAG system)         â”‚
â”‚                                                         â”‚
â”‚  USAGE:                                                 â”‚
â”‚  â€¢ Offline analysis only                                â”‚
â”‚  â€¢ NOT used during inference                            â”‚
â”‚  â€¢ Complements LLM-as-a-Judge evaluation                â”‚
â”‚  â€¢ Provides industry-standard metrics                   â”‚
â”‚                                                         â”‚
â”‚  OUTPUT:                                                â”‚
â”‚  â€¢ ragas_results.json (detailed scores)                 â”‚
â”‚  â€¢ ragas_visualization.png (charts)                     â”‚
â”‚  â€¢ Summary metrics (printed)                            â”‚
â”‚                                                         â”‚
â”‚  INTEGRATION:                                           â”‚
â”‚  â€¢ GUI "Run RAGAS Evaluation" button                    â”‚
â”‚  â€¢ Automated evaluation pipeline                        â”‚
â”‚  â€¢ Comparison with LLM-as-a-Judge                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ **Files**

| File | Purpose |
|------|---------|
| `ragas_eval.py` | Main evaluation script |
| `ragas_metrics.py` | Metric definitions |
| `visualize_results.py` | Result visualization |
| `ragas_results.json` | Evaluation results (generated) |
| `ragas_visualization.png` | Charts (generated) |
| `requirements.txt` | Dependencies |
| `evaluation-ragas-explained.md` | This documentation |

---

## ğŸ¯ **Key Takeaways**

```
1. RAGAS = RETRIEVAL-AUGMENTED GENERATION ASSESSMENT:
   Automated evaluation framework for RAG systems.

2. 4 KEY METRICS:
   â€¢ Context Recall: Did we retrieve ground truth?
   â€¢ Context Precision: Are retrieved chunks relevant?
   â€¢ Faithfulness: Is answer grounded (no hallucination)?
   â€¢ Answer Relevancy: Does answer address question?

3. LLM AS EVALUATOR:
   Uses gpt-4o-mini to judge quality (semantic understanding).

4. FRAMEWORK-AGNOSTIC:
   Works with any RAG system (LlamaIndex, LangChain, custom).

5. OFFLINE ANALYSIS:
   NOT used during inference.
   Batch evaluation of test suite.

6. COMPLEMENTS LLM-AS-A-JUDGE:
   Two perspectives = comprehensive evaluation.

7. 7-STEP PIPELINE:
   Load â†’ Initialize â†’ Query â†’ Build Dataset â†’ Evaluate â†’ Save

8. INTERPRETING SCORES:
   0.9-1.0: Excellent âœ…
   0.7-0.9: Good ğŸŸ¡
   0.5-0.7: Moderate âš ï¸
   0.0-0.5: Poor âŒ

9. CONFIGURATION:
   Tune retriever settings (top_k, similarity_threshold)
   based on precision/recall trade-offs.

10. OUTPUT:
    JSON results + visualization + summary metrics.
```

---

**Built for RagAgentv2 - Auto Claims RAG System** ğŸ“ŠğŸ”¬

ğŸ¯ Key Takeaways:

1. RAGAS = RETRIEVAL-AUGMENTED GENERATION ASSESSMENT:
   Automated evaluation framework for RAG systems.
   Uses LLM (gpt-4o-mini) as intelligent evaluator.

2. 4 KEY METRICS:
   â€¢ Context Recall: "Did we retrieve ground truth?"
     Example: Ground truth "555-1234" in retrieved chunks? YES â†’ 1.0
   
   â€¢ Context Precision: "Are retrieved chunks relevant?"
     Example: 2 relevant, 1 irrelevant in top-3 â†’ 0.67
   
   â€¢ Faithfulness: "Is answer grounded? (No hallucination?)"
     Example: All answer claims supported by context â†’ 1.0
   
   â€¢ Answer Relevancy: "Does answer address question?"
     Example: Answer directly addresses phone question â†’ 0.95

3. HOW IT WORKS (7 STEPS):
   1. Load test cases (test_cases.json)
   2. Initialize RAG system (index, agents)
   3. Query RAG system (get answers + contexts)
   4. Build RAGAS dataset (HuggingFace format)
   5. Initialize evaluator LLM (gpt-4o-mini)
   6. Run evaluation (each metric, each question)
   7. Save results (ragas_results.json)

4. LLM AS EVALUATOR:
   Why better than BLEU/ROUGE:
   âœ… Understands semantic similarity
   âœ… Can detect hallucination
   âœ… Can reason about relevance
   âœ… Human-like judgment

5. FRAMEWORK-AGNOSTIC:
   Works with ANY RAG system:
   â€¢ LlamaIndex âœ…
   â€¢ LangChain âœ…
   â€¢ Custom RAG âœ…
   Only needs: question, answer, contexts, ground_truth

6. OFFLINE ANALYSIS ONLY:
   NOT used during inference!
   Batch evaluation of test suite.
   No latency impact on users.

7. VS. LLM-AS-A-JUDGE:
   LLM-as-a-Judge: Custom, domain-specific, Gemini
   RAGAS: Standard, general RAG, OpenAI
   â†’ Use BOTH for comprehensive evaluation!

8. SCORE INTERPRETATION:
   0.9-1.0: Excellent ğŸŸ¢ (System performing very well)
   0.7-0.9: Good ğŸŸ¡ (Room for improvement)
   0.5-0.7: Moderate âš ï¸ (Significant issues)
   0.0-0.5: Poor ğŸ”´ (Critical issues)

9. CONFIGURATION:
   needle_retriever: top_k=3, threshold=0.75 (precision)
   map_reduce: top_k=15 (recall)
   LLM: gpt-4o-mini, temp=0.0, timeout=60s

10. OUTPUT:
    â€¢ ragas_results.json (detailed scores per question)
    â€¢ ragas_visualization.png (charts)
    â€¢ Summary metrics (printed)
    â€¢ GUI integration (compare evaluations)