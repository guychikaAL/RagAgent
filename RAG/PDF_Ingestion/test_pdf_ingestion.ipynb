{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Ingestion Layer - Test Notebook\n",
        "\n",
        "This notebook tests the PDF Ingestion Layer in isolation.\n",
        "\n",
        "## Purpose\n",
        "- Load a real PDF from the project\n",
        "- Run the ingestion pipeline\n",
        "- Inspect the output Document\n",
        "- Validate metadata extraction\n",
        "- Verify text quality\n",
        "\n",
        "## What This Tests\n",
        "‚úÖ PDF loading and validation  \n",
        "‚úÖ Text extraction quality  \n",
        "‚úÖ Text normalization  \n",
        "‚úÖ Metadata extraction  \n",
        "‚úÖ LlamaIndex Document creation  \n",
        "\n",
        "## What This Does NOT Test\n",
        "‚ùå Chunking (that's Layer 2)  \n",
        "‚ùå Embeddings (that's Layer 3)  \n",
        "‚ùå Retrieval (that's Layer 3)  \n",
        "‚ùå Agents (that's Layer 4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/guyai/Desktop/AI Lecture/FIRST PROJECT/RagAgentv2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().absolute().parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PDF Ingestion module imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import the ingestion pipeline\n",
        "from RAG.PDF_Ingestion.pdf_ingestion import create_ingestion_pipeline, PDFIngestionError\n",
        "\n",
        "print(\"‚úÖ PDF Ingestion module imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 1: Basic Ingestion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with PDF: /Users/guyai/Desktop/AI Lecture/FIRST PROJECT/RagAgentv2/auto_claim_20_forms_FINAL.pdf\n",
            "File exists: True\n",
            "File size: 44.9 KB\n"
          ]
        }
      ],
      "source": [
        "# Path to test PDF\n",
        "pdf_path = project_root / \"auto_claim_20_forms_FINAL.pdf\"\n",
        "\n",
        "print(f\"Testing with PDF: {pdf_path}\")\n",
        "print(f\"File exists: {pdf_path.exists()}\")\n",
        "print(f\"File size: {pdf_path.stat().st_size / 1024:.1f} KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Pipeline created\n"
          ]
        }
      ],
      "source": [
        "# Create ingestion pipeline\n",
        "pipeline = create_ingestion_pipeline(document_type=\"insurance_claim_form\")\n",
        "\n",
        "print(\"‚úÖ Pipeline created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ingestion successful!\n",
            "Document type: <class 'llama_index.core.schema.Document'>\n",
            "Document ID: 6e1c9a74673919ad\n"
          ]
        }
      ],
      "source": [
        "# Run ingestion\n",
        "try:\n",
        "    document = pipeline.ingest(str(pdf_path))\n",
        "    print(\"‚úÖ Ingestion successful!\")\n",
        "    print(f\"Document type: {type(document)}\")\n",
        "    print(f\"Document ID: {document.doc_id}\")\n",
        "except PDFIngestionError as e:\n",
        "    print(f\"‚ùå Ingestion failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 2: Inspect Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã DOCUMENT METADATA\n",
            "============================================================\n",
            "{\n",
            "  \"document_id\": \"6e1c9a74673919ad\",\n",
            "  \"document_type\": \"insurance_claim_form\",\n",
            "  \"source_file\": \"auto_claim_20_forms_FINAL.pdf\",\n",
            "  \"source_path\": \"/Users/guyai/Desktop/AI Lecture/FIRST PROJECT/RagAgentv2/auto_claim_20_forms_FINAL.pdf\",\n",
            "  \"title\": \"auto_claim_20_forms_FINAL\",\n",
            "  \"language\": \"en\",\n",
            "  \"page_count\": 40,\n",
            "  \"total_characters\": 25417,\n",
            "  \"total_words\": 3641,\n",
            "  \"total_paragraphs\": 40,\n",
            "  \"avg_paragraph_length\": 91.0,\n",
            "  \"has_headings\": false,\n",
            "  \"dates_detected\": [\n",
            "    \"2024-06-06\",\n",
            "    \"2024-07-08\",\n",
            "    \"2024-11-16\",\n",
            "    \"2024-06-08\",\n",
            "    \"2024-07-30\",\n",
            "    \"2024-08-20\",\n",
            "    \"2024-08-01\",\n",
            "    \"2024-02-14\",\n",
            "    \"2024-02-25\",\n",
            "    \"2024-02-16\"\n",
            "  ],\n",
            "  \"times_detected\": [\n",
            "    \"14:59\",\n",
            "    \"15:15\",\n",
            "    \"15:33\",\n",
            "    \"09:29\",\n",
            "    \"09:32\",\n",
            "    \"09:31\",\n",
            "    \"10:12\",\n",
            "    \"14:54\",\n",
            "    \"15:20\",\n",
            "    \"10:59\"\n",
            "  ],\n",
            "  \"numeric_density\": \"medium\",\n",
            "  \"ingested_at\": \"2025-12-13T10:56:37.198602Z\",\n",
            "  \"ingestion_pipeline_version\": \"1.0\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Display all metadata\n",
        "import json\n",
        "\n",
        "print(\"üìã DOCUMENT METADATA\")\n",
        "print(\"=\" * 60)\n",
        "print(json.dumps(document.metadata, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç METADATA VALIDATION\n",
            "============================================================\n",
            "‚úÖ document_id\n",
            "‚úÖ document_type\n",
            "‚úÖ source_file\n",
            "‚úÖ title\n",
            "‚úÖ language\n",
            "‚úÖ total_words\n",
            "‚úÖ total_characters\n",
            "‚úÖ total_paragraphs\n",
            "‚úÖ avg_paragraph_length\n",
            "‚úÖ has_headings\n",
            "‚úÖ dates_detected\n",
            "‚úÖ times_detected\n",
            "‚úÖ numeric_density\n",
            "‚úÖ ingested_at\n",
            "\n",
            "‚úÖ All required metadata fields present\n"
          ]
        }
      ],
      "source": [
        "# Validate required metadata fields\n",
        "required_fields = [\n",
        "    \"document_id\",\n",
        "    \"document_type\",\n",
        "    \"source_file\",\n",
        "    \"title\",\n",
        "    \"language\",\n",
        "    \"total_words\",\n",
        "    \"total_characters\",\n",
        "    \"total_paragraphs\",\n",
        "    \"avg_paragraph_length\",\n",
        "    \"has_headings\",\n",
        "    \"dates_detected\",\n",
        "    \"times_detected\",\n",
        "    \"numeric_density\",\n",
        "    \"ingested_at\",\n",
        "]\n",
        "\n",
        "print(\"\\nüîç METADATA VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "missing_fields = []\n",
        "for field in required_fields:\n",
        "    if field in document.metadata:\n",
        "        print(f\"‚úÖ {field}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {field} - MISSING\")\n",
        "        missing_fields.append(field)\n",
        "\n",
        "if missing_fields:\n",
        "    print(f\"\\n‚ö†Ô∏è  Missing {len(missing_fields)} required fields\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All required metadata fields present\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 3: Inspect Text Quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä TEXT STATISTICS\n",
            "============================================================\n",
            "Total characters: 25,417\n",
            "Total words: 3,641\n",
            "Total lines: 79\n",
            "Total paragraphs: 40\n"
          ]
        }
      ],
      "source": [
        "# Display text statistics\n",
        "print(\"üìä TEXT STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total characters: {len(document.text):,}\")\n",
        "print(f\"Total words: {len(document.text.split()):,}\")\n",
        "print(f\"Total lines: {len(document.text.split(chr(10))):,}\")\n",
        "print(f\"Total paragraphs: {len([p for p in document.text.split(chr(10)*2) if p.strip()]):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ FIRST 500 CHARACTERS OF CLEAN TEXT\n",
            "============================================================\n",
            "AUTO CLAIM FORM #1 BlueRiver Mutual SECTION 1 ‚Äì CLAIMANT INFORMATION Name: Jon Mor Account Number: ACC9900057 Address: 100 Main Street, Sample City, ST 90000 Phone: (555) 100-2000 Email: jon.mor@example.com Date of Incident: 2024-06-06 Location: 10th Ave & 5th St, Sample City Injury: Yes (minor) Police Report: Yes SECTION 2 ‚Äì CLAIM DETAILS Accident Type: Rear-end collision Severity: Minor Claim Status: Pending court Fraud Risk Score: 4 Internal Tag: TOW-FLAG-3 Assigned Adjuster: Linda Cooper SEC\n",
            "\n",
            "[... text continues ...]\n"
          ]
        }
      ],
      "source": [
        "# Display first 500 characters\n",
        "print(\"\\nüìÑ FIRST 500 CHARACTERS OF CLEAN TEXT\")\n",
        "print(\"=\" * 60)\n",
        "print(document.text[:500])\n",
        "print(\"\\n[... text continues ...]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ LAST 500 CHARACTERS OF CLEAN TEXT\n",
            "============================================================\n",
            "[... text continues ...]\n",
            "\n",
            "Witness Statement: Witness reported seeing the other car accelerate abruptly. Repair Estimate 1: $1260 Repair Estimate 2: $1645 Repair Shop Assigned: Horizon Collision Repair Repair Appointment Date: 2025-09-04\n",
            "\n",
            "Hidden Note: Second witness: **Laura Vance** SECTION 5 ‚Äì MINI TIMELINE OF EVENTS No timeline available for this claim. SECTION 6 ‚Äì COURT DATE Court Date: N/A SECTION 7 ‚Äì DECLARATION I declare that the information provided isaccurate. Signature: __________________________ Date: 2025-08-09\n"
          ]
        }
      ],
      "source": [
        "# Display last 500 characters\n",
        "print(\"\\nüìÑ LAST 500 CHARACTERS OF CLEAN TEXT\")\n",
        "print(\"=\" * 60)\n",
        "print(\"[... text continues ...]\\n\")\n",
        "print(document.text[-500:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample paragraphs\n",
        "paragraphs = [p.strip() for p in document.text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "print(f\"\\nüìÑ SAMPLE PARAGRAPHS (showing 3 of {len(paragraphs)})\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, para in enumerate(paragraphs[:3], 1):\n",
        "    print(f\"\\nParagraph {i}:\")\n",
        "    print(\"-\" * 60)\n",
        "    # Show first 300 chars of paragraph\n",
        "    if len(para) > 300:\n",
        "        print(para[:300] + \"...\")\n",
        "    else:\n",
        "        print(para)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 4: Text Quality Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç TEXT QUALITY CHECKS\n",
            "============================================================\n",
            "‚úÖ Text is not empty\n",
            "‚úÖ Text has reasonable length\n",
            "‚úÖ No excessive newlines\n",
            "‚úÖ No form feed characters\n",
            "‚úÖ Low ratio of single-character words (0.6%)\n",
            "‚úÖ Text has paragraph structure\n",
            "\n",
            "‚úÖ All quality checks passed!\n"
          ]
        }
      ],
      "source": [
        "# Check for common PDF extraction issues\n",
        "print(\"üîç TEXT QUALITY CHECKS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "issues = []\n",
        "\n",
        "# Check 1: Text is not empty\n",
        "if not document.text.strip():\n",
        "    issues.append(\"‚ùå Text is empty\")\n",
        "else:\n",
        "    print(\"‚úÖ Text is not empty\")\n",
        "\n",
        "# Check 2: Text has reasonable length\n",
        "if len(document.text) < 100:\n",
        "    issues.append(\"‚ùå Text is suspiciously short\")\n",
        "else:\n",
        "    print(\"‚úÖ Text has reasonable length\")\n",
        "\n",
        "# Check 3: No excessive whitespace\n",
        "if '\\n\\n\\n' in document.text:\n",
        "    issues.append(\"‚ö†Ô∏è  Text contains excessive newlines (3+)\")\n",
        "else:\n",
        "    print(\"‚úÖ No excessive newlines\")\n",
        "\n",
        "# Check 4: No form feed characters\n",
        "if '\\f' in document.text:\n",
        "    issues.append(\"‚ùå Text contains form feed characters\")\n",
        "else:\n",
        "    print(\"‚úÖ No form feed characters\")\n",
        "\n",
        "# Check 5: Words are properly formed (no excessive single chars)\n",
        "words = document.text.split()\n",
        "single_char_words = sum(1 for w in words if len(w) == 1 and w.isalpha())\n",
        "single_char_ratio = single_char_words / len(words) if words else 0\n",
        "if single_char_ratio > 0.1:\n",
        "    issues.append(f\"‚ö†Ô∏è  High ratio of single-character words ({single_char_ratio:.1%})\")\n",
        "else:\n",
        "    print(f\"‚úÖ Low ratio of single-character words ({single_char_ratio:.1%})\")\n",
        "\n",
        "# Check 6: Has paragraph structure\n",
        "if '\\n\\n' not in document.text:\n",
        "    issues.append(\"‚ö†Ô∏è  Text has no paragraph breaks\")\n",
        "else:\n",
        "    print(\"‚úÖ Text has paragraph structure\")\n",
        "\n",
        "# Summary\n",
        "if issues:\n",
        "    print(f\"\\n‚ö†Ô∏è  Found {len(issues)} quality issues:\")\n",
        "    for issue in issues:\n",
        "        print(f\"  {issue}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All quality checks passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç DOCUMENT OBJECT VALIDATION\n",
            "============================================================\n",
            "‚úÖ Document is a LlamaIndex Document instance\n",
            "‚úÖ Document has 'text' attribute\n",
            "‚úÖ Document has 'metadata' attribute\n",
            "‚úÖ Document has 'doc_id' attribute\n",
            "   doc_id: 6e1c9a74673919ad\n",
            "‚úÖ doc_id matches metadata.document_id\n"
          ]
        }
      ],
      "source": [
        "# Verify it's a proper LlamaIndex Document\n",
        "from llama_index.core import Document as LlamaDocument\n",
        "\n",
        "print(\"üîç DOCUMENT OBJECT VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check type\n",
        "if isinstance(document, LlamaDocument):\n",
        "    print(\"‚úÖ Document is a LlamaIndex Document instance\")\n",
        "else:\n",
        "    print(f\"‚ùå Document is not a LlamaIndex Document (type: {type(document)})\")\n",
        "\n",
        "# Check required attributes\n",
        "if hasattr(document, 'text'):\n",
        "    print(\"‚úÖ Document has 'text' attribute\")\n",
        "else:\n",
        "    print(\"‚ùå Document missing 'text' attribute\")\n",
        "\n",
        "if hasattr(document, 'metadata'):\n",
        "    print(\"‚úÖ Document has 'metadata' attribute\")\n",
        "else:\n",
        "    print(\"‚ùå Document missing 'metadata' attribute\")\n",
        "\n",
        "if hasattr(document, 'doc_id'):\n",
        "    print(\"‚úÖ Document has 'doc_id' attribute\")\n",
        "    print(f\"   doc_id: {document.doc_id}\")\n",
        "else:\n",
        "    print(\"‚ùå Document missing 'doc_id' attribute\")\n",
        "\n",
        "# Check doc_id matches metadata\n",
        "if document.doc_id == document.metadata.get('document_id'):\n",
        "    print(\"‚úÖ doc_id matches metadata.document_id\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  doc_id does not match metadata.document_id\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 6: Error Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç ERROR HANDLING TESTS\n",
            "============================================================\n",
            "\n",
            "Test: Non-existent file\n",
            "‚úÖ Correctly raised PDFIngestionError: PDF file does not exist: nonexistent.pdf\n",
            "\n",
            "Test: Non-PDF file\n",
            "‚úÖ Correctly raised PDFIngestionError: PDF file does not exist: requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# Test with non-existent file\n",
        "print(\"üîç ERROR HANDLING TESTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nTest: Non-existent file\")\n",
        "try:\n",
        "    pipeline.ingest(\"nonexistent.pdf\")\n",
        "    print(\"‚ùå Should have raised PDFIngestionError\")\n",
        "except PDFIngestionError as e:\n",
        "    print(f\"‚úÖ Correctly raised PDFIngestionError: {e}\")\n",
        "\n",
        "print(\"\\nTest: Non-PDF file\")\n",
        "try:\n",
        "    pipeline.ingest(\"requirements.txt\")\n",
        "    print(\"‚ùå Should have raised PDFIngestionError\")\n",
        "except PDFIngestionError as e:\n",
        "    print(f\"‚úÖ Correctly raised PDFIngestionError: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "This notebook has tested the PDF Ingestion Layer in isolation.\n",
        "\n",
        "### What We Verified:\n",
        "1. ‚úÖ PDF file loading and validation\n",
        "2. ‚úÖ Text extraction from PDF\n",
        "3. ‚úÖ Text normalization and cleaning\n",
        "4. ‚úÖ Metadata extraction (document-level)\n",
        "5. ‚úÖ LlamaIndex Document object creation\n",
        "6. ‚úÖ Error handling for invalid inputs\n",
        "\n",
        "### Next Layer:\n",
        "**Layer 2: Chunking**\n",
        "- Will take this Document as input\n",
        "- Will create hierarchical nodes (Sections ‚Üí Parent ‚Üí Child)\n",
        "- Will enrich metadata with chunk-level information\n",
        "- Still NO embeddings or vector stores\n",
        "\n",
        "### Notes:\n",
        "- This layer is COMPLETE and ISOLATED\n",
        "- It has NO dependencies on chunking, indexing, or agents\n",
        "- It produces a clean Document ready for Layer 2\n",
        "- All metadata is lightweight and document-level only\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ragagent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
