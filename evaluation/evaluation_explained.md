# LLM-as-a-Judge Evaluation - Complete Guide

## ğŸ“Š **What is LLM-as-a-Judge?**

LLM-as-a-Judge is an automated evaluation method where an **independent AI model** (Gemini) evaluates the quality of your RAG system's answers. It's like having a teacher grade your homework - but the teacher is another AI that doesn't know what you're trying to do, so it's fair!

---

## ğŸ¯ **The Big Picture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LLM-AS-A-JUDGE EVALUATION                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1. Test Cases (questions + ground truth)              â”‚
â”‚            â†“                                            â”‚
â”‚  2. Run through YOUR RAG system (OpenAI)               â”‚
â”‚            â†“                                            â”‚
â”‚  3. Collect: answer, route, chunks, confidence         â”‚
â”‚            â†“                                            â”‚
â”‚  4. Send to GEMINI JUDGE (independent evaluator)       â”‚
â”‚            â†“                                            â”‚
â”‚  5. Judge scores 3 metrics (A, B, C)                   â”‚
â”‚            â†“                                            â”‚
â”‚  6. Aggregate scores â†’ Final evaluation                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ **The 7-Step Evaluation Pipeline**

### **Step 1: Load Test Cases**

Test cases are stored in `test_cases.json`:

```json
{
  "id": "q1",
  "type": "needle",
  "question": "What is Jon Mor's phone number?",
  "ground_truth": "555-1234",
  "expected_chunks": ["chunk_id_1", "chunk_id_2"]
}
```

**What each test case contains:**
- âœ… **Question**: What to ask the RAG system
- âœ… **Ground Truth**: The correct answer
- âœ… **Expected Chunks**: Which chunks should be retrieved
- âœ… **Type**: NEEDLE (specific fact) or SUMMARY (broad question)

---

### **Step 2: Run Through RAG System**

Each question is processed by your RAG system:

```python
# Your system processes the question
result = orchestrator.run("What is Jon Mor's phone number?")

# Returns:
{
  "answer": "555-1234",
  "route": "NEEDLE",
  "sources": ["chunk_id_1"],
  "confidence": 0.95,
  "retrieved_chunks_content": ["Chunk text..."]
}
```

**What happens internally:**
1. âœ… **Router Agent** decides: NEEDLE or SUMMARY
2. âœ… **Retriever** finds relevant chunks from vector database
3. âœ… **Agent** (Needle or Summary) generates answer
4. âœ… **System** returns answer + metadata

---

### **Step 3: Collect Results**

For EACH test case, the system collects:

| Data Point | Example | Purpose |
|------------|---------|---------|
| **System Answer** | "555-1234" | What your RAG returned |
| **Route** | "NEEDLE" | Which agent was used |
| **Retrieved Chunks** | ["chunk_1", "chunk_2"] | What chunks were used |
| **Confidence** | 0.95 | System's confidence score |
| **Chunk Content** | "Jon Mor: 555-1234" | Actual text of chunks |

---

### **Step 4: Initialize Gemini Judge**

```python
judge = GeminiJudge(model="gemini-2.5-flash")
```

**Why use Gemini instead of OpenAI?**

```
RAG System â†’ Uses OpenAI (gpt-4o-mini)
Judge      â†’ Uses Gemini (gemini-2.5-flash)
             â†‘ Different model = Unbiased evaluation
```

**Key Benefits:**
- âœ… **Independent perspective**: Not "marking its own homework"
- âœ… **Avoids bias**: Different model, different training data
- âœ… **Unbiased scoring**: Doesn't favor OpenAI-style answers
- âœ… **Industry standard**: Best practice for RAG evaluation

---

### **Step 5: Evaluate 3 Metrics**

The judge evaluates **3 separate metrics** for each test case. Each metric gets a score of **0.0**, **0.5**, or **1.0**.

---

#### **ğŸ“Š Metric A: ANSWER CORRECTNESS**

**Question:** *"Did the system give the right answer?"*

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT TO JUDGE:                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Question: "What is Jon Mor's phone?"    â”‚
â”‚ Ground Truth: "555-1234"                â”‚
â”‚ System Answer: "555-1234"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ GEMINI JUDGE â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT:                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Score: 1.0 (Fully Correct)              â”‚
â”‚ Explanation: "System answer matches     â”‚
â”‚              ground truth exactly."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scoring Guidelines:**

| Score | Meaning | Criteria |
|-------|---------|----------|
| **1.0** | Fully Correct | âœ… Matches ground truth exactly<br>âœ… All key facts present<br>âœ… No contradictions<br>âœ… No hallucinations |
| **0.5** | Partially Correct | âš ï¸ Some facts match, some missing<br>âš ï¸ Correct direction but incomplete<br>âš ï¸ Minor contradictions |
| **0.0** | Incorrect | âŒ Wrong answer<br>âŒ Contradicts ground truth<br>âŒ Hallucinates facts<br>âŒ Says "I don't know" when answer exists |

**Judge Prompt (Simplified):**
```
You are an ANSWER CORRECTNESS evaluator.

Compare:
1. GROUND TRUTH (correct answer)
2. SYSTEM ANSWER (what RAG returned)

Rules:
âš ï¸ DO NOT use external knowledge
âš ï¸ ONLY compare what is explicitly stated
âš ï¸ Be CONSERVATIVE - penalize weak matches

Return ONLY JSON:
{
  "score": 1.0 or 0.5 or 0.0,
  "explanation": "..."
}
```

---

#### **ğŸ“Š Metric B: CONTEXT RELEVANCY**

**Question:** *"Did the system use relevant chunks?"*

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT TO JUDGE:                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Question: "What is Jon Mor's phone?"    â”‚
â”‚ Question Type: "needle"                 â”‚
â”‚ Retrieved Chunks:                       â”‚
â”‚   - "Jon Mor, phone: 555-1234"          â”‚
â”‚   - "Address: 123 Main St"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ GEMINI JUDGE â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT:                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Score: 1.0 (Highly Relevant)            â”‚
â”‚ Explanation: "Chunks contain phone      â”‚
â”‚              number info, relevant."    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scoring Guidelines:**

| Score | Meaning | Criteria |
|-------|---------|----------|
| **1.0** | Highly Relevant | âœ… All chunks semantically related to question<br>âœ… Chunks contain needed information<br>âœ… No irrelevant chunks<br>âœ… Appropriate chunk types for question |
| **0.5** | Partially Relevant | âš ï¸ Some chunks relevant, some not<br>âš ï¸ Relevant info mixed with noise<br>âš ï¸ Suboptimal chunk types |
| **0.0** | Not Relevant | âŒ Chunks don't address question<br>âŒ Wrong topic entirely<br>âŒ No useful information |

**What This Metric Evaluates:**

For **NEEDLE questions** (specific facts):
- Should retrieve: Atomic, precise child chunks
- Focus: High precision (exact facts)
- Example: "What is the phone number?" â†’ Chunk with phone number only

For **SUMMARY questions** (broad context):
- Should retrieve: Broader parent/merged chunks
- Focus: High recall (comprehensive context)
- Example: "Describe the incident" â†’ Multiple chunks with event details

**Judge Prompt (Simplified):**
```
You are a CONTEXT RELEVANCY evaluator.

Evaluate whether the system used the RIGHT chunks.

Rules:
âš ï¸ Evaluate ONLY semantic relevance to question
âš ï¸ DO NOT evaluate if answer was correct
âš ï¸ Be STRICT - penalize irrelevant chunks

Return ONLY JSON:
{
  "score": 1.0 or 0.5 or 0.0,
  "explanation": "..."
}
```

---

#### **ğŸ“Š Metric C: CONTEXT RECALL**

**Question:** *"Did the system retrieve the expected chunks?"*

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT TO JUDGE:                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Question: "What is Jon Mor's phone?"    â”‚
â”‚ Expected Chunks:                        â”‚
â”‚   - chunk_id_1                          â”‚
â”‚   - chunk_id_2                          â”‚
â”‚ Actually Retrieved:                     â”‚
â”‚   - chunk_id_1                          â”‚
â”‚   - chunk_id_5                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ GEMINI JUDGE â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT:                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Score: 0.5 (Partial Recall)             â”‚
â”‚ Explanation: "Got chunk_id_1 but        â”‚
â”‚              missed chunk_id_2."        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scoring Guidelines:**

| Score | Meaning | Criteria |
|-------|---------|----------|
| **1.0** | Perfect Recall | âœ… ALL expected chunks retrieved<br>âœ… No missing necessary chunks |
| **0.5** | Partial Recall | âš ï¸ SOME expected chunks retrieved<br>âš ï¸ Some necessary chunks missing |
| **0.0** | No Recall | âŒ NONE of expected chunks retrieved<br>âŒ All necessary chunks missing |

**Important Notes:**
- âœ… This metric is **independent** of answer correctness
- âœ… It only checks: "Were the right chunks retrieved?"
- âœ… Even if answer is correct, recall can be low (if wrong chunks were used)
- âœ… Even if answer is wrong, recall can be high (if right chunks were retrieved)

**Judge Prompt (Simplified):**
```
You are a CONTEXT RECALL evaluator.

Evaluate whether the expected chunks were retrieved.

Rules:
âš ï¸ Compare retrieved chunks to expected chunks
âš ï¸ DO NOT evaluate answer quality
âš ï¸ ONLY check if expected chunks are present

Return ONLY JSON:
{
  "score": 1.0 or 0.5 or 0.0,
  "explanation": "..."
}
```

---

### **Step 6: Aggregate Scores**

For each test case, compute the **final score** as the average of all 3 metrics:

```python
final_score = (
    answer_correctness.score +
    context_relevancy.score +
    context_recall.score
) / 3
```

**Example Calculation:**

```
Test Case: "What is Jon Mor's phone number?"

â”œâ”€ A. Answer Correctness: 1.0  (âœ… Correct answer)
â”œâ”€ B. Context Relevancy:  1.0  (âœ… Relevant chunks)
â”œâ”€ C. Context Recall:     0.5  (âš ï¸ Partial recall)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Final Score: (1.0 + 1.0 + 0.5) / 3 = 0.83
```

**Why Average All Three?**
- âœ… Captures **multiple dimensions** of quality
- âœ… A system can have correct answers (Metric A) but poor retrieval (Metric C)
- âœ… Forces the system to perform well across **all aspects**
- âœ… Prevents "gaming" the evaluation by optimizing only one metric

---

### **Step 7: Save Results**

Results are saved to `evaluation_results.json`:

```json
{
  "question_id": "q1",
  "question": "What is Jon Mor's phone number?",
  "question_type": "needle",
  "ground_truth": "555-1234",
  "system_answer": "555-1234",
  "route": "NEEDLE",
  "retrieved_chunks": ["chunk_id_1"],
  "confidence": 0.95,
  
  "answer_correctness": {
    "score": 1.0,
    "explanation": "System answer matches ground truth exactly."
  },
  "context_relevancy": {
    "score": 1.0,
    "explanation": "All retrieved chunks are relevant to the question."
  },
  "context_recall": {
    "score": 0.5,
    "explanation": "Retrieved chunk_id_1 but missed chunk_id_2."
  },
  
  "final_score": 0.83
}
```

---

## ğŸ¯ **Key Design Principles**

### **1. Independent Judge (No Bias)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG System: OpenAI (gpt-4o-mini)    â”‚
â”‚            â†“ Different Models â†“     â”‚
â”‚ Judge:      Gemini (gemini-2.5-flash)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ Unbiased Evaluation
```

**Why This Matters:**
- âŒ **BAD**: Using OpenAI to judge OpenAI â†’ Biased, "marking own homework"
- âœ… **GOOD**: Using Gemini to judge OpenAI â†’ Independent, unbiased perspective

---

### **2. Read-Only Evaluation**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluation is EXTERNAL              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Does NOT modify RAG system        â”‚
â”‚ âœ… Does NOT affect live answers      â”‚
â”‚ âœ… Offline evaluation post-factum    â”‚
â”‚ âœ… Safe to run anytime               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What This Means:**
- âœ… You can run evaluation **without breaking anything**
- âœ… Evaluation **doesn't change** your system's behavior
- âœ… It's purely **observational** and **analytical**

---

### **3. Strict JSON Responses**

The judge **MUST** return valid JSON in this exact format:

```json
{
  "score": 1.0,  // Must be 0.0, 0.5, or 1.0
  "explanation": "Brief 2-3 sentence explanation"
}
```

**Why This Matters:**
- âœ… **Structured output** â†’ Easy to parse and analyze
- âœ… **No free-form text** â†’ Prevents judge from going off-topic
- âœ… **Consistent format** â†’ Reliable automation
- âœ… **Type safety** â†’ Scores are always numbers

---

### **4. Conservative Scoring**

The judge is instructed to be **strict** and **conservative**:

```
âœ… Penalizes weak matches
âœ… Requires explicit evidence
âœ… No external knowledge allowed
âœ… No generous assumptions
âœ… Must see facts explicitly stated
```

**Why This Matters:**
- âœ… **High standards** â†’ Ensures quality
- âœ… **Prevents false positives** â†’ Don't reward lucky guesses
- âœ… **Reproducible** â†’ Same input â†’ Same score
- âœ… **Trustworthy** â†’ Results reflect true quality

---

## ğŸ“Š **Complete Evaluation Flow Example**

Let's walk through a **complete example** step-by-step:

### **Test Case:**
```json
{
  "id": "q1",
  "question": "What is Jon Mor's phone number?",
  "ground_truth": "555-1234",
  "expected_chunks": ["chunk_1", "chunk_2"]
}
```

---

### **Step 1: RAG System Runs**

```python
result = orchestrator.run("What is Jon Mor's phone number?")

# Returns:
{
  "answer": "555-1234",
  "route": "NEEDLE",
  "sources": ["chunk_1"],
  "confidence": 0.95,
  "retrieved_chunks_content": ["Jon Mor, phone: 555-1234"]
}
```

---

### **Step 2: Gemini Judge Evaluates**

#### **Metric A: Answer Correctness**

```
Input:
  Question: "What is Jon Mor's phone number?"
  Ground Truth: "555-1234"
  System Answer: "555-1234"

Judge Analysis:
  âœ… System answer matches ground truth exactly
  âœ… No missing information
  âœ… No hallucinations

Output:
  Score: 1.0 âœ…
  Explanation: "System answer matches ground truth exactly."
```

---

#### **Metric B: Context Relevancy**

```
Input:
  Question: "What is Jon Mor's phone number?"
  Question Type: "needle"
  Retrieved Chunks:
    - "Jon Mor, phone: 555-1234"

Judge Analysis:
  âœ… Chunk contains phone number information
  âœ… Directly relevant to question
  âœ… No irrelevant information

Output:
  Score: 1.0 âœ…
  Explanation: "Chunk contains exactly the information needed."
```

---

#### **Metric C: Context Recall**

```
Input:
  Question: "What is Jon Mor's phone number?"
  Expected Chunks: ["chunk_1", "chunk_2"]
  Retrieved Chunks: ["chunk_1"]

Judge Analysis:
  âœ… Retrieved chunk_1 (expected)
  âŒ Did NOT retrieve chunk_2 (expected)
  âš ï¸ Partial recall

Output:
  Score: 0.5 âš ï¸
  Explanation: "Retrieved chunk_1 but missed chunk_2."
```

---

### **Step 3: Final Score Computation**

```
Final Score = (A + B + C) / 3
            = (1.0 + 1.0 + 0.5) / 3
            = 0.83
```

---

### **Step 4: Results Summary**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Test Case q1 Results                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Question: "What is Jon Mor's phone number?"  â•‘
â•‘  Route: NEEDLE                                â•‘
â•‘                                               â•‘
â•‘  A. Answer Correctness:  1.0 âœ…               â•‘
â•‘     "Exact match with ground truth"           â•‘
â•‘                                               â•‘
â•‘  B. Context Relevancy:   1.0 âœ…               â•‘
â•‘     "All chunks relevant"                     â•‘
â•‘                                               â•‘
â•‘  C. Context Recall:      0.5 âš ï¸               â•‘
â•‘     "Partial recall, missed chunk_2"          â•‘
â•‘                                               â•‘
â•‘  Final Score:            0.83                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸš€ **How to Run Evaluation**

### **Command:**

```bash
cd evaluation
python run_evaluation.py
```

### **What Happens:**

```
[1/7] Loading test cases...
âœ… Loaded 8 test cases

[2/7] Initializing RAG system...
âœ… RAG system ready

[3/7] Running test cases through RAG system...
  [1/8] q1: What is Jon Mor's phone number?
    Route: NEEDLE
    Answer: 555-1234...
âœ… Collected 8 RAG outputs

[4/7] Initializing Gemini judge...
âœ… Judge initialized

[5/7] Running evaluation...
  Evaluating q1...
  Evaluating q2...
  ...
âœ… Evaluation complete

[6/7] Displaying results...
ğŸ“Š EVALUATION RESULTS
q1: What is Jon Mor's phone number?
  A. Answer Correctness: 1.0
  B. Context Relevancy:  1.0
  C. Context Recall:     0.5
  Final Score:           0.83

ğŸ“ˆ AVERAGE SCORES
  A. Answer Correctness: 0.88
  B. Context Relevancy:  0.91
  C. Context Recall:     0.75
  Final Score:           0.85

[7/7] Saving results...
âœ… Results saved to: evaluation_results.json
```

---

## ğŸ“ **Files Involved**

| File | Purpose |
|------|---------|
| `run_evaluation.py` | Main script - orchestrates entire evaluation |
| `evaluator.py` | Core evaluation logic and Gemini judge |
| `judge_prompts.py` | Prompt templates for each metric |
| `test_cases.json` | Test questions + ground truth answers |
| `evaluation_results.json` | Output - evaluation results |

---

## ğŸ“ **Understanding the Metrics**

### **Why 3 Separate Metrics?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric A: Answer Correctness                â”‚
â”‚   â†’ Measures: "Did we get the right answer?"â”‚
â”‚   â†’ Focus: End result quality                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric B: Context Relevancy                  â”‚
â”‚   â†’ Measures: "Did we use relevant chunks?"  â”‚
â”‚   â†’ Focus: Retrieval quality                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric C: Context Recall                     â”‚
â”‚   â†’ Measures: "Did we find expected chunks?" â”‚
â”‚   â†’ Focus: Retrieval completeness            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Each metric catches different failures:**

| Scenario | A | B | C | Problem Detected |
|----------|---|---|---|------------------|
| Perfect system | 1.0 | 1.0 | 1.0 | âœ… Everything works |
| Lucky guess | 1.0 | 0.0 | 0.0 | âš ï¸ Right answer, wrong chunks |
| Good retrieval, bad synthesis | 0.0 | 1.0 | 1.0 | âš ï¸ Got chunks, wrong answer |
| Incomplete retrieval | 0.5 | 0.5 | 0.5 | âš ï¸ Partial everything |
| Completely broken | 0.0 | 0.0 | 0.0 | âŒ Nothing works |

---

## ğŸ’¡ **Key Insights**

### **1. Why Independent Model Matters**

**Bad Example (Biased):**
```
System: "What is X?"
OpenAI: "X is Y"

Judge (also OpenAI): "This sounds like something I would say! âœ…"
â†’ BIASED evaluation
```

**Good Example (Unbiased):**
```
System: "What is X?"
OpenAI: "X is Y"

Judge (Gemini): "Let me check if this matches ground truth..."
â†’ INDEPENDENT evaluation
```

---

### **2. Why Conservative Scoring Matters**

**Without Conservative Scoring:**
```
Ground Truth: "555-1234"
System Answer: "5551234" (missing dash)

Lenient Judge: "Close enough! Score: 1.0"
â†’ HIDES formatting issues
```

**With Conservative Scoring:**
```
Ground Truth: "555-1234"
System Answer: "5551234" (missing dash)

Strict Judge: "Not exact match. Score: 0.5"
â†’ CATCHES formatting issues
```

---

### **3. Why Read-Only Matters**

```
âœ… GOOD: Evaluation observes system externally
   â†’ Safe, reproducible, doesn't break anything

âŒ BAD: Evaluation modifies system
   â†’ Dangerous, unreliable, could break production
```

---

## âœ… **Summary**

**LLM-as-a-Judge** is:
- âœ… An **independent AI** (Gemini) evaluating your RAG system
- âœ… **Automated** - no manual checking needed
- âœ… **Consistent** - same inputs â†’ same scores
- âœ… **Unbiased** - uses different model than your system
- âœ… **Multi-dimensional** - evaluates 3 aspects of quality
- âœ… **Scalable** - can evaluate 100+ test cases
- âœ… **Actionable** - provides explanations for debugging

**It works by:**
1. Running test questions through your RAG system
2. Collecting answers and retrieved chunks
3. Asking Gemini to score 3 metrics (0.0, 0.5, 1.0)
4. Computing final score as average
5. Providing detailed explanations

**Why it's powerful:**
- ğŸ¯ Catches issues regular testing might miss
- ğŸ¯ Measures not just correctness, but also **how** you got the answer
- ğŸ¯ Independent perspective prevents overfitting to your model's style
- ğŸ¯ Actionable feedback helps you improve specific components

---

## ğŸ“ **Analogy**

Think of it like a school exam:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Your RAG System = Student                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Test Cases = Exam Questions                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemini Judge = Teacher (different school)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metrics = Grading Rubric                    â”‚
â”‚   A. Answer Correctness = Did you answer?   â”‚
â”‚   B. Context Relevancy  = Did you cite?     â”‚
â”‚   C. Context Recall     = All sources used? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The teacher (Gemini):**
- âŒ Didn't train the student (no bias)
- âœ… Grades based on rubric (consistent)
- âœ… Checks work, not just answer (comprehensive)
- âœ… Provides feedback (actionable)

---

## ğŸ“š **Further Reading**

- `run_evaluation.py` - Main evaluation script
- `evaluator.py` - Core evaluation logic
- `judge_prompts.py` - Detailed prompt templates
- `test_cases.json` - Example test cases
- `evaluation_results.json` - Sample output

---

**Built for RagAgentv2 - Auto Claims RAG System** ğŸš—

evaluation/evaluation_explained.md
â”œâ”€ ğŸ“Š What is LLM-as-a-Judge?
â”œâ”€ ğŸ¯ The Big Picture (Visual Flow)
â”œâ”€ ğŸ”„ 7-Step Evaluation Pipeline
â”‚   â”œâ”€ Step 1: Load Test Cases
â”‚   â”œâ”€ Step 2: Run Through RAG System
â”‚   â”œâ”€ Step 3: Collect Results
â”‚   â”œâ”€ Step 4: Initialize Gemini Judge
â”‚   â”œâ”€ Step 5: Evaluate 3 Metrics
â”‚   â”‚   â”œâ”€ Metric A: Answer Correctness
â”‚   â”‚   â”œâ”€ Metric B: Context Relevancy
â”‚   â”‚   â””â”€ Metric C: Context Recall
â”‚   â”œâ”€ Step 6: Aggregate Scores
â”‚   â””â”€ Step 7: Save Results
â”œâ”€ ğŸ¯ Key Design Principles
â”œâ”€ ğŸ“Š Complete Evaluation Flow Example
â”œâ”€ ğŸš€ How to Run Evaluation
â”œâ”€ ğŸ“ Files Involved
â”œâ”€ ğŸ“ Understanding the Metrics
â”œâ”€ ğŸ’¡ Key Insights
â”œâ”€ âœ… Summary
â””â”€ ğŸ“ School Exam Analogy